# Data Modeling with Regression {#regression}

```{r setup_reg, include=FALSE, purl=FALSE}
chap <- 6
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth'
  )
options(scipen = 99, digits = 4)

# This bit of code is a bug fix on asis blocks, which we use to show/not show LC
# solutions, which are written like markdown text. In theory, it shouldn't be
# necessary for knitr versions <=1.11.6, but I've found I still need to for
# everything to knit properly in asis blocks. More info here: 
# https://stackoverflow.com/questions/32944715/conditionally-display-block-of-markdown-text-using-knitr
library(knitr)
knit_engines$set(asis = function(options) {
  if (options$echo && options$eval) knit_child(text = options$code)
})

# This controls which LC solutions to show. Options for solutions_shown: "ALL"
# (to show all solutions), or subsets of c('5-1', '5-2','5-3', '5-4'), including
# the null vector c('') to show no solutions.
solutions_shown <- c('')
show_solutions <- function(section){
  return(solutions_shown == "ALL" | section %in% solutions_shown)
  }
```

Now that we are equipped with data visualization skills from Chapter \@ref(viz), data wrangling skills from Chapter \@ref(wrangling), and an understanding of the "tidy" data format from Chapter \@ref(tidy), we now proceed to discuss once of the most commonly used statistical procedures: *regression*.  Much as we saw with the Grammar of Graphics in Chapter \@ref(viz), the fundamental premise of (simple linear) regression is to *model* the relationship between 

* An outcome/dependent/predicted variable $y$
* As a function of a covariate/independent/predictor variable $x$

Why do we have multiple labels for the same concept? Whatâ€™s their root? Regression, in its simplest form, can be viewed in two ways:

1. **For Prediction**: You want to predict an outcome variable $y$ based on the information contained in a set of predictor variables. You don't care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about $y$, you're fine. 
1. **For Explanation**: You want to study the relationship between an outcome variable $y$ and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these.

In this chapter, we use the `flights` data frame in the `nycflights13` package to look at the relationship between departure delay, arrival delay, and other variables related to flights.  We will also discuss the concept of *correlation* and how it is frequently incorrectly implied to also lead to *causation*. This chapter also introduces the `broom` package, which is a useful tool for summarizing the results of regression fits in "tidy" format.

### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r message=FALSE, warning=FALSE}
library(nycflights13)
library(ggplot2)
library(dplyr)
library(broom)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(mvtnorm) 
library(tidyr)
```






## Datasets

### Professor evaluations

A study conducted at the University of Texas in Austin
available at [openintro.org](https://www.openintro.org/stat/data/?data=evals) 
investigated associations between teacher evalutions and
various attributes of the instructor. 

* **Question**: Can we explain a professor's teaching evaluation score using these
attritubes?
* Outcome variable $y$: Average teaching score, based on students evaluations between 1 and 5
* Explanatory variables $x$
    + their rank: teaching, tenure track, or tenured
    + their ethnicity: minority or non-minority
    + their (binary) gender: male or female
    + their language: whether or not english was their mother tongue
    + their age:
    + their average "beauty" rating, based on a panel of 6 students' scores between 1 and 10.

First let's load the data and look at a random sample of 5 rows:

```{r}
load(url("http://www.openintro.org/stat/data/evals.RData"))
evals <- evals %>%
  select(score, ethnicity, gender, language, age, bty_avg, rank)
```

```{r, echo=FALSE}
evals %>% 
  sample_n(5) %>% 
  kable(
    digits=2,
    caption = "Random sample of 5 instructors", 
    booktabs = TRUE
  )
```


### Country-by-country life expectancies in 2007 {#datalifeexp}

The `gapminder` package contains the `gapminder` data frame, which contains
development data about various countries for 5-year intervals between 1952 and 2007

* **Question**: What variables are associated with life expectancy for all
countries in 2007? In other words, can we identify variables that can explain
variation in life expectancy between countries.
* Outcome variable $y$: Mean life expectancy
* Explanatory variables $x$
    + continent
    + GDP per capita

First let's load the data and look at a random sample of 5 rows:

```{r}
library(gapminder)
gapminder2007 <- gapminder %>%
  filter(year == 2007) %>% 
  select(country, lifeExp, continent, gdpPercap)
```

```{r, echo=FALSE}
gapminder2007 %>% 
  sample_n(5) %>% 
  kable(digits=1)
```



### Credit card balance {#datacreditcard}

The `ISLR` package contains the `Credit` data frame, which contains information
on Credit balances. Unfortunately, I don't have information on which population
this data was drawn from, so we can't generalize any of these results.

* **Question**: What variables are associated with how much credit card debt
an individual has
* Outcome variable $y$ `Balance`
* Explanatory variables
    + $x_1$ `Income`: Income in $10000's
    + $x_2$ `Limit`: Credit limit

First let's load the data and look at a random sample of 5 rows:

```{r}
library(ISLR)
Credit <- Credit %>%
  select(Balance, Income, Limit)
```

```{r, echo=FALSE}
Credit %>% 
  sample_n(5) %>% 
  kable(digits=1)
```





## One numerical x: Professor evaluations

### Exploratory data analysis

Let's

* Model the outcome variable y: teacher evaluation score
* Using explanatory variable x: beauty score

Let's plot the data:

```{r numxplot1, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score")
```

The above suffers from *overplotting*; let's break it up with a little random jitter added to the points. Note, we are
only altering the visualization of the points; the original data is the same.

```{r numxplot2, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: Jittered"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_jitter() + 
  labs(x="Beauty Score", y="Teaching Score")
```


### Correlation coefficient

The correlation coefficient between teacher evaluation score and beauty score is
high:

```{r, eval=FALSE}
cor(evals$score, evals$bty_avg)
```
```{r, echo=FALSE}
evals %>% 
  summarize(correlation = cor(score, bty_avg)) %>% 
  kable(
    digits=3,
    caption = "Correlation between teaching score and beauty score", 
    booktabs = TRUE
  )
```


### Simple linear regression

Let's

* Fit a simple linear regression using the command `lm(y ~ x, data)` below:
* Output the table in tidy data format using the `tidy()` function from the `broom` package.

```{r, eval=FALSE}
lm(score~bty_avg, data=evals) %>% 
  tidy()
```
```{r, echo=FALSE}
lm(score~bty_avg, data=evals) %>% 
  tidy() %>% 
  kable(
    digits=3,
    caption = "Regression line estimates", 
    booktabs = TRUE
  )
```

Let's plot the regression line with a new layer called `+ geom_smooth(method="lm")`:

```{r numxplot3, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: With regression line"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm")
```

Let's ignore the grey bars now (which are "error" bars) by adding the argument `se=FALSE` to `geom_smooth`

```{r numxplot4, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: With regression line, no error bars"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE)
```



### Observed values, fitted values, and residuals

```{r numxplot5, echo=FALSE, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: Example of fitted value and residual"}
x <- 7.333
y <- 4.9
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat

ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE) +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
```

* Red point: *Observed point* $(x,y) = (\mbox{beauty score}, \mbox{teaching score}) = (7.333, 4.9)$. *Observed value* $y=4.9$
* Red square: *Fitted point* $(x,\widehat{y}) = (\mbox{beauty score}, \mbox{fitted teaching score}) = (7.333, 4.369)$. *Fitted value* $\widehat{y} = 4.369$. Why 4.371? From table above: $\widehat{y} = b_0 + b_1 x = 3.880 + 0.067 \times 7.333 = 4.369$
* Length of blue arrow: *Residual* (i.e. error) $y - \widehat{y} = 4.9 - 4.369 = 0.531$

Let's extract point-by-point information now:

* for all `r nrow(evals)` points in our plot, in other words
* for all `r nrow(evals)` instructors in the study, in other words
* for all `r nrow(evals)` rows in the `evals` data frame

using the `augment()` function from the `broom` package. This function
*augments* the original data by adding new information resulting from the
regression. Let's look at 5 arbitrarily chosen rows. (Note that the first row in
the table corresponds to the red points and blue arrow in Figure
\@ref(fig:numxplot5)):

```{r, eval=FALSE}
point_by_point_info <- lm(score~bty_avg, data=evals) %>% 
  augment() %>% 
  select(score, bty_avg, .fitted, .resid)
```
```{r, echo=FALSE}
set.seed(76)
point_by_point_info <- lm(score~bty_avg, data=evals) %>% 
  augment() %>% 
  select(score, bty_avg, .fitted, .resid)
point_by_point_info %>% 
  slice(c(21, sample(1:nrow(evals), 4))) %>% 
  kable(
    digits=3,
    caption = "Point-by-point information (5 of 463 rows)", 
    booktabs = TRUE
  )
```

The periods in the variable names indicated that they are new information that
was added to the original dataset. In particular

* **.fitted**: the fitted value $\widehat{y}$ corresponding to an observed point $(x,y)$. One 
example is the red square in Figure \@ref(fig:numxplot5)
* **.resid**: the residual $y - \widehat{y}$. One example is the length of the blue arrow
in Figure \@ref(fig:numxplot5).




### Residual analysis {#residual_analysis}

Recall the residuals can be thought of as "lack-of-fit", or "left-overs", or
"errors". Ideally, we want there to be *no systematic pattern* to the residuals. 
What does this mean?

1. The residuals should be on average 0. 
1. The spread of the residuals should be consistent.
1. The value of the residuals should not depend on the value of x

Let's look at the residuals two ways. First, recall in Figure \@ref(fig:numxplot5)
above we plotted:

* On the y-axis: $y$ teaching score
* On the x-axis: $x$ beauty score

Instead, in Figure \@ref(fig:numxplot6) below let's plot

* On the y-axis: $y-\widehat{y}$ residual instead
* On the x-axis: $x$ beauty score (same as before)

```{r numxplot6, echo=FALSE, warning=FALSE, fig.cap="Plot of residuals over beauty score"}
ggplot(point_by_point_info, aes(x=bty_avg, y=.resid)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Residual") +
  geom_hline(yintercept=0, col="blue", size =1) +
  annotate("point", x=x, y=resid, col="red", size=3) +
  annotate("point", x=x, y=0, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=resid, yend=0, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
```

You can think of Figure \@ref(fig:numxplot6) as Figure \@ref(fig:numxplot5), but
with the blue line flattened out. Does it seem like there is *no systematic pattern*
to the residuals?

Here are some hypothetical examples where there is a systematic pattern to the
residuals:

```{r numxplot7, echo=FALSE, warning=FALSE, fig.cap="Examples of less than ideal residual patterns"}
temp <- evals
temp$`Example 1` <- ((evals$bty_avg-5)^2 - 6 + rnorm(nrow(evals), 0, 0.5))*0.4
temp$`Example 2` <- (rnorm(nrow(evals), 0, 0.075*evals$bty_avg^2))*0.4

temp <- temp %>%
  select(bty_avg, `Example 1`, `Example 2`) %>% 
  gather(type, eps, -bty_avg)

ggplot(temp, aes(x=bty_avg, y=eps)) +
  geom_point() + 
  labs(x="Beauty Score", y="Residual") +
  geom_hline(yintercept=0, col="blue", size =1) +
  facet_wrap(~type)
```

The second way to look at the residuals is using a histogram:

```{r numxplot8, warning=FALSE, fig.cap="Histogram of residuals"}
ggplot(point_by_point_info, aes(x=.resid)) +
  geom_histogram(binwidth = 0.25) +
  labs(x="Residual")
```

This histogram seems to indicate that we have more positive residuals than negative.
Since Residual  = $y-\widehat{y} > 0$ when $y > \widehat{y}$, it seems
our fitted professor score from the regression model have a slight tendency of
being small than the actually observed score.

This histogram has a slight *left-skew* in that there is a long tail on the
left. Another way to say this is this data exhibit a *negative skew*. Here are examples
of an ideal and less than ideal patterns of residuals.

```{r numxplot9, echo=FALSE, warning=FALSE, fig.cap="Examples of ideal and less than ideal residual patterns"}
temp <- evals
temp$`Ideal` <- rnorm(nrow(temp), 0, sd=sd(point_by_point_info$.resid))
temp$`Less than ideal` <- rnorm(nrow(temp), 0, sd=sd(point_by_point_info$.resid))^2

temp <- temp %>%
  select(bty_avg, `Ideal`, `Less than ideal`) %>% 
  gather(type, eps, -bty_avg)

ggplot(temp, aes(x=eps)) +
  geom_histogram(binwidth = 0.25) +
  labs(x="Residual") +
  facet_wrap(~type, scales = "free")
```

In fact, we'll see later on that we would like the residuals to be *normally distributed* with
mean 0. In other words, be bell-shaped and centered at 0!



### Best fitting lines

Regression lines are also known as "best fitting lines". But what do we mean by best? Let's unpack the criteria
that is used by regression to determine best. Recall the plot in Figure \@ref(fig:numxplot5)) where for a professor
with a beauty average score of $x=7.333$

* The observed value $y=4.9$ was marked with a red circle
* The fitted value $\widehat{y} = 4.369$ on the regression line was marked with a red square
* The residual $y-\widehat{y} = 4.9-4.369 = 0.531$ was the length of the blue arrow.

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=2.333$. The residual in this case is $2.7 - 4.036 = -1.336$.

```{r echo=FALSE}
x <- 7.333
y <- 4.9
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat

best_fit_plot <- ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE) +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 

x <- 2.333
y <- 2.7
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat
best_fit_plot <- best_fit_plot +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
best_fit_plot
```

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=3.667$. The residual in this case is $4.4 - 4.125 = 0.2753$.

```{r echo=FALSE}
x <- 3.667
y <- 4.4
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat
best_fit_plot <- best_fit_plot +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
best_fit_plot
```

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=6$. The residual in this case is $3.8 - 4.28 = -0.4802$.

```{r echo=FALSE}
x <- 6
y <- 3.8
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat
best_fit_plot <- best_fit_plot +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
best_fit_plot
```


Now let's say we repeated this process for all `r nrow(evals)` professors in our
dataset. Regression *minimizes the sum of all `r nrow(evals)` arrow lengths
squared.* In other words, it minimizes the sum of the squared residuals:

$$
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
$$

We square the arrow lengths so that positive and negative deviations of the same amount are treated equally.  That's why alternative names for the simple linear regression line are the **least-squares line** and the **best fitting line**. It can be proven via calculus and linear algebra that this line uniquely minimizes the sum of the squared arrow lengths. 

For the regression line in the plot, the sum of the squared residuals is `r lm(score~bty_avg, data=evals) %>% augment() %>% summarise(SSE = sum(.resid^2)) %>% pull(SSE) %>% round(3)`.



 





## One categorical x: Professor evaluations

### Exploratory data analysis

Let's load the country-by-country life expectancy `gapminder2007` data frame
that we created in the "Datasets" Section \@ref(datalifeexp).

```{r, eval=TRUE}
lifeExp_by_continent <- gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(median = median(lifeExp))
```
```{r, echo=FALSE}
lifeExp_by_continent %>% 
  kable(
    digits=2,
    caption = "Life expectancy by continent", 
    booktabs = TRUE
  )
lifeExp_by_continent <- gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(median = median(lifeExp))

# What was the median life expectancy in Africa in 2007?
median_africa <- lifeExp_by_continent %>% 
  filter(continent == "Africa") %>%
  pull(median)
```

The median life expectancy in Africa in was 52.93. Let's look at a boxplot of
life expectancy and mark the median for Africa in red.

```{r catxplot1, warning=FALSE, fig.cap="Life expectancy in 2007"}
ggplot(gapminder2007, aes(x=continent, y=lifeExp)) +
  geom_boxplot() +
  geom_hline(yintercept = 52.93, col="red", size=1) +
  labs(x = "Continent", y="Life expectancy (years)")
```

Let's look number of years difference relative to Africa's median.

```{r catxplot2, warning=FALSE, fig.cap="Difference in life expectancy relative to African median of 52.93 years"}
ggplot(gapminder2007, aes(x=continent, y=lifeExp-52.93)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, col="red", size=1) +
  labs(x="Continent", y="Difference in life expectancy (years)")
```


### Simple linear regression

Note that while the boxplots in Figure \@ref(fig:catxplot2)) involved medians,
regression involves means. Using the `gapminder2007` data frame again, let's compute
*mean* life expectancy instead of median.

```{r, eval=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp))
```
```{r, echo=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp)) %>% 
  kable(
    digits=2,
    caption = "Continent means", 
    booktabs = TRUE
  )
```

Let's now compute the *regression table*. Recall:

* `lm(lifeExp ~ continent, data=gapminder2007)` is a regression where
    + $y$ is the numerical outcome variable life expectancy
    + $x$ is the categorical explanatory/predictor variable continent
* We then pipe this into `tidy()` from the `broom` package. All this does is
output the regression table in a clean format; "tidy data" format as seen in
Chapter \@ref(tidy) in fact!
* You can think of the following two lines of code as a `generate_regression_table()` function:

```{r, eval=FALSE}
lm(lifeExp ~ continent, data=gapminder2007) %>% 
  tidy()
```
```{r, echo=FALSE}
lm(lifeExp ~ continent, data=gapminder2007) %>% 
  tidy() %>% 
  kable(
    digits=2,
    caption = "Regression table", 
    booktabs = TRUE
  )
```

What are these values? Recall:

* Africa is treated as the "baseline for comparison".
    + The intercept of 54.81 corresponds to the mean life expectancy of Africa.
    + It was chosen for no other reason than it is first alphabetically. You can
    manually specify which continent to use as baseline, but we leave that to a more
    advanced course.
* continentAmericas was created by the regression.
    + It is the additional mean life expectancy of the Americas *above and beyond
    the baseline of comparison which is Africa*. Note that 18.80 = 73.61 - 54.81,
    the difference in mean life expectancies of the Americas and Africa from the 
    table of continent means above.
* Same goes for Asia, Europe, and Oceania
* What are std.error, statistic, and p.value? We'll see this later in the course
after we've seen statistical inference, but here's a preview:
    + std.error is the standard error: it quantifies the uncertainy
    + statistic and p.value: these inform us on the *statistical significance*

To really bring home what the values in the regression table are, let's subtract
the mean life expectancy for Africa of 54.81 from each of the 5 mean life expectancies
we computed in the table above and add a new column.

```{r, eval=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp)) %>% 
  mutate(compare_to_africa = round(mean - 54.81,2))
```
```{r, echo=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp)) %>% 
  mutate(compared_to_africa = mean - 54.81) %>% 
  kable(
    digits=2,
    caption = "Continent means and comparison of means relative to baseline", 
    booktabs = TRUE
  )
```


### Observed values, fitted values, and residuals

Let's extract point-by-point information now:

* for all `r nrow(gapminder2007)` points in the five boxplots in Figure \@ref(fig:catxplot2)) 
* for all `r nrow(gapminder2007)` countries in our dataset
* for all `r nrow(gapminder2007)` rows in the `gapminder2007` data frame

```{r, eval=FALSE}
point_by_point_info <- lm(lifeExp ~ continent, data=gapminder2007) %>% 
  augment() %>% 
  select(lifeExp, continent, .fitted, .resid)
```

Note:

* `lm(lifeExp ~ continent, data=gapminder2007)` is the regression from earlier
* We then pipe this into `augment()` from the `broom` package. All this does is
output point-by-point information in a clean format; "tidy data" format as seen
in Chapter \@ref(tidy) in fact!
* We then only select the `lifeExp`, `continent`, `.fitted`, and  `.resid` columns.
We don't need the rest, so let's keep our output clean.

Let's look at a random sample of 5 of these `r nrow(gapminder2007)` rows:

```{r, echo=FALSE}
set.seed(76)
point_by_point_info <- lm(lifeExp ~ continent, data=gapminder2007) %>% 
  augment() %>% 
  select(lifeExp, continent, .fitted, .resid)
point_by_point_info %>% 
  sample_n(5) %>% 
  kable(
    digits=3,
    caption = "Point-by-point information (5 of 142 rows)", 
    booktabs = TRUE
  )
```


### Residual analysis

Recall our discussion on residuals from Section \@ref(residual_analysis) where there were
two ways to visualize the residuals to see if there is a *systematic pattern* to them:

1. A plot with residuals on the y-axis and the predictor (in this case continent) on the x-axis
1. A histogram of all residuals

First, let's plot the residuals vs continent using `geom_jitter()` again because 
of overplotting issues. Note here we control the amount of jitter horizontally
by setting the `width` argument. Doesn't seem to bad!

```{r catxplot7, warning=FALSE, fig.cap="Plot of residuals over continent"}
ggplot(point_by_point_info, aes(x=continent, y=.resid)) + 
  geom_jitter(width=0.1) + 
  labs(x="Continent", y="Residual") +
  geom_hline(yintercept=0, col="blue", size =1) 
```

However, there is one clear outlier in Asia. It has the smallest residual, hence
also has the smallest life expectancy in Asia. Let's see who the 5 countries
with the shortest life expectancy are:

```{r, eval=FALSE}
gapminder2007 %>% 
  filter(continent == "Asia") %>% 
  select(country, lifeExp) %>% 
  arrange(lifeExp)
```
```{r, echo=FALSE}
gapminder2007 %>% 
  filter(continent == "Asia") %>% 
  select(country, lifeExp) %>% 
  arrange(lifeExp) %>%
  slice(1:5) %>% 
  kable(
    digits=3,
    caption = "Countries in Asia with shortest life expectancy", 
    booktabs = TRUE
  )
```


Second, let's look at a histogram of all `r nrow(gapminder2007)` residuals:

```{r catxplot8, warning=FALSE, fig.cap="Histogram of residuals"}
ggplot(point_by_point_info, aes(x=.resid)) +
  geom_histogram(binwidth = 5) +
  labs(x="Residual")
```

Looks pretty good!



 





## Two numerical x: Credit card balance

We now move from *simple linear regression*, which involves a single
explanatory/predictor variable, to *multiple regression*, which involves more
than one explanatory/predictor variable.

### Exploratory data analysis

Let's load the Credit card balance `Credit` data frame
that we created in the "Datasets" Section \@ref(datacreditcard).

Now that we have two numerical predictors, an ideal way to visualize this data
is with a 3D scatterplot. Unfortunately we can't show such a plot in this book, but
you can run the following code in R to create an interactive 3D plot using the `plotly` package.

```{r, eval=FALSE}
library(ISLR)
library(plotly)
plot_ly(showscale=FALSE) %>%
  add_markers(
    x = Credit$Income,
    y = Credit$Limit,
    z = Credit$Balance,
    hoverinfo = 'text',
    text = ~paste("x1 - Income: ", Credit$Income, 
                  "</br> x2 - Limit: ", Credit$Limit, 
                  "</br> y - Balance: ", Credit$Balance)
  ) %>% 
  layout(
    scene = list(
      xaxis = list(title = "x1 - Income (in $10K)"),
      yaxis = list(title = "x2 - Limit ($)"),
      zaxis = list(title = "y - Balance ($)")
    )
  )
```

Let's do the next best thing and look at two separate scatterplots of the outcome
variable $y$ credit card balance in dollars with

1. Explanatory/predictor variable $x$ being credit limit in dollars
1. Explanatory/predictor variable $x$ being income in $10,000 units

```{r 2numxplot1, warning=FALSE, fig.cap="Relationship between credit card balance and credit limit"}
ggplot(Credit, aes(x=Limit, y=Balance)) +
  geom_point() +
  labs(x="Credit limit (in $)", y="Credit card balance (in $)") +
  geom_smooth(method="lm", se=FALSE)
```

```{r 2numxplot2, warning=FALSE, fig.cap="Relationship between credit card balance and credit limit"}
ggplot(Credit, aes(x=Income, y=Balance)) +
  geom_point() +
  labs(x="Income (in $10K)", y="Credit card balance (in $)") +
  geom_smooth(method="lm", se=FALSE)
```

In both cases, there seems to be a positive relationship. Let's look at the 
correlation coefficient in both cases. 

```{r, eval=FALSE}
cor(Credit$Limit, Credit$Balance)
```
```{r, echo=FALSE}
Credit %>% 
  summarize(correlation = cor(Limit, Balance)) %>% 
  kable(
    digits=3,
    caption = "Correlation between credit limit and credit card balance", 
    booktabs = TRUE
  )
```


```{r, eval=FALSE}
cor(Credit$Income, Credit$Balance)
```
```{r, echo=FALSE}
Credit %>% 
  summarize(correlation = cor(Income, Balance)) %>% 
  kable(
    digits=3,
    caption = "Correlation between income (in $10K) and credit card balance", 
    booktabs = TRUE
  )
```

In both cases, the correlation coefficient indicates a strong linear relationship,
in particular between credit limit and credit card balance! Makes sense! Note 
however there were a few individuals who had balances of 0!

What if in instead we looked the correlation coefficient between income and
credit card balance, where income was in dollars and not tens of thousands of dollars? i.e. we 
multiply $Income$ by 10,000?

```{r, eval=FALSE}
cor(Credit$Income * 10000, Credit$Balance)
```
```{r, echo=FALSE}
Credit %>% 
  summarize(correlation = cor(Income, Balance)) %>% 
  kable(
    digits=3,
    caption = "Correlation between income (in $) and credit card balance", 
    booktabs = TRUE
  )
```

We see it is the same! We say that correlation coefficient is invariant to linear
transformations! In other words

* The correlation between $x$ and $y$ will be the same as
* The correlation between $x$ and $a\times y + b$ where $a, b$ are numerical values (real numbers in mathematical terms)


### Multiple regression

Let's now compute the *regression table*

```{r, eval=FALSE}
lm(Balance ~ Limit + Income, data=Credit) %>% 
  tidy()
```
```{r, echo=FALSE}
lm(Balance ~ Limit + Income, data=Credit) %>% 
  tidy() %>% 
  kable(
    digits=2,
    caption = "Regression table", 
    booktabs = TRUE
  )
```

Whereas in simple linear regression with a single numerical predictor, for
multiple linear regression with two numerical predictors, the regression line
is now a *regression plane*. In other words, a flat surface! See the
interactive 3D scatterplot with a regression plane added [here](https://rudeboybert.github.io/STAT135/index.html#36_multiple_regression).

How do we interpret the three values that define the plane?

* Intercept: -$385.18. In this case, the intercept doesn't have a practical
interpretation as no one can have a credit card balance of less than $0. It is
used merely to situation the plane.
* Limit: $0.26. Now that we have multiple variables to consider, we have to add
a caveat to our interpretation: *all other things being equal, for every increase in one dollar in credit limit, there is an associated increase of on average 26 cents in credit card balance*. Note:
    + Again, we are not making any causal statements, only statements relating to the association between credit limit and balance
    + The *all other things being equal* related to the other explanatory/predictor variables, in this case only one: Income. This is equivalent to saying "holding income constant, we observed an associated increase of $0.26 in credit card balance for every dollar increase in credit limit"
* Income: -$7.66. Negative? Say what? Didn't we see in Figure
\@ref(fig:2numxplot2)) that the relationship was positive and that the
correlation between Income and credit card balance was positive?

