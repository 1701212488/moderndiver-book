# Data Modeling with Regression {#regression}

```{r setup_reg, include=FALSE, purl=FALSE}
chap <- 6
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE,
  out.width = "\\textwidth"
  )
options(scipen = 99, digits = 3)

# This bit of code is a bug fix on asis blocks, which we use to show/not show LC
# solutions, which are written like markdown text. In theory, it shouldn't be
# necessary for knitr versions <=1.11.6, but I've found I still need to for
# everything to knit properly in asis blocks. More info here: 
# https://stackoverflow.com/questions/32944715/conditionally-display-block-of-markdown-text-using-knitr
library(knitr)
knit_engines$set(asis = function(options) {
  if (options$echo && options$eval) knit_child(text = options$code)
})

# This controls which LC solutions to show. Options for solutions_shown: "ALL"
# (to show all solutions), or subsets of c('5-1', '5-2','5-3', '5-4'), including
# the null vector c("") to show no solutions.
solutions_shown <- c("")
show_solutions <- function(section){
  return(solutions_shown == "ALL" | section %in% solutions_shown)
  }
```

Now that we are equipped with data visualization skills from Chapter \@ref(viz), data wrangling skills from Chapter \@ref(wrangling), and an understanding of the "tidy" data format from Chapter \@ref(tidy), we now proceed with data modeling. The fundamental premise of data modeling is *to model the relationship* between:

* An outcome variable $y$, also called a dependent variable
* Explantory/predictor variables $\vec{x}$, also called independent variables and covariates. The arrow on top of the $x$ indicates that we have a *vector* or a series of values.

Why do we have two different labels, explanatory and predictor, for the variables $\vec{x}$? That's because data modeling can be viewed through two lenses

1. **Modeling for explanation**: You want to study the relationship between an outcome variable $y$ and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these. For example: "Does knowing a population's smoking habits explain their lung cancer prevalence?"
1. **Modeling for prediction**: You want to predict an outcome variable $y$ based on the information contained in a set of predictor variables. You don't care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about $y$, you're fine. For example: "Can we predict whether someone will enjoy a recommended movie based on their previous movie ratings?"

Data modeling is used in a wide variety of fields, including statistical inference, causal inference, artificial intelligence, and machine learning. There are many techniques for data modeling, such as tree-based models, neural networks/deep learning, and more. However, we'll focus on one particular technique: *linear regression*, one of the most commonly-used and easy to understand approaches to modeling. Linear regression involves:

* An outcome variable $y$ that is *numerical*
* Explantory/predictor variables $\vec{x}$ that are either *numerical* or *categorical*

In this chapter, we'll start considering a wider array of datasets, all easily accessible via R packages. We will also discuss the concept of *correlation* and how it is frequently incorrectly used to imply *causation*. 

### Needed packages {-}

Let's load all the packages needed for this chapter. If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r neededpackages, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(moderndive)
```

Notice we load a new package called `moderndive` thus you'll need to install this once. This is an accompaniment package to the ModernDive book that includes 3 useful functions for linear regression:

1. `get_regression_table()`: Fit a linear regression and get the *regression table*
1. `get_regression_points()`: Fit a linear regression and get all the relevant points, including the *fitted value* $\widehat{y}$ and *residuals*
1. `get_regression_summaries()`: Fit a linear regression and get summary statistics about the regression, including values called $R^2$ and $R^2$-adjusted.

```{r internalpackages, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(mvtnorm)
library(tidyr)
library(forcats)
```




## Five Regression Models - The 5RM

As stated in the introduction, linear regression can be used to model the relationship between

* An outcome variable $y$ that is *numerical*
* Explantory/predictor variables $\vec{x}$ that are either *numerical* or *categorical*

Whereas there is always only one numerical outcome varible $y$, we have choices on both the number and the type of explantory/predictor variables $\vec{x}$ to use. In this Chapter, we're going to cover five regression models which we term the "5RM":

1. 5RM#1: A single numerical explantory/predictor variable $x$. This scenario is known as *simple linear regression*. We'll be using the `evals` dataset of instructor evaluations at the University of Texas, Austin.
1. 5RM#2: A single categorical explantory/predictor variable $x$. We'll be using the `gapminder` dataset of international development data.
1. 5RM#3: Two numerical explantory/predictor variables $x_1$ and $x_2$. This is the first scenario of *multiple regression* given that there are now more than one explantory/predictor variable. We'll be using the `Credit` dataset of credit card balance data. 
1. 5RM#4: One numerical and one categorical explantory/predictor variable. We'll cover *interaction models* here. We'll revisit the `evals` dataset here. 
1. 5RM#5: Two categorical explantory/predictor variables. We'll be using the `biopics` dataset of movie data. 




## 5RM#1: Understanding Teacher Evaluations {#model1}

Why do some professors at universities and colleges get high teaching evaluations from students while others don't? What factors can explain these differences? Are there biases? These are questions that are interest to professors and administrators, as teaching evaluations are among the criteria considered for promotion to tenure.

Researchers at the University of Texas in Austin tried to answer this question: what factors can explain differences in instructor's teaching evaluation scores? To this end, the collected information on $n=463$ instructors. A full description of the study can be found at [openintro.org](https://www.openintro.org/stat/data/?data=evals), but let's summarize the variables that were collected:

* Outcome variable $y$: Average teaching score, based on students evaluations between 1 and 5
* Explanatory variables $x$
    + their rank: teaching, tenure track, or tenured
    + their ethnicity: minority or non-minority
    + their (binary) gender: male or female
    + their language: whether or not english was their mother tongue
    + their age:
    + their average "beauty" rating, based on a panel of 6 students' scores between 1 and 10.

Let's load the data and `select()` only the 7 variables listed above:

```{r}
load(url("http://www.openintro.org/stat/data/evals.RData"))
evals <- evals %>%
  select(score, bty_avg, ethnicity, gender, language, age, rank)
```


In this section we'll try to explain differences in instructor evaluations scores as a function of their beauty scores. We'll model the relationship between these two variables with a particular kind of linear regression called *simple linear regression*. Simple linear regression is the most basic form of linear regression where we have

1. A numerical outcome variable $y$, in this case teaching `score`
1. A single numerical explanatory/predictor variable $x$, in this case `bty_avg`

### Exploratory data analysis {#model1EDA}

A crucial step before doing any kind of modeling or analysis however is performing an *exploratory data analysis*, or EDA, of all our data. EDA can give you a sense of the distribution of data, whether there are outliers and/or missing values, but most importantly it can inform how to build your model. There are many aproaches to EDA, here are three:

1. Most fundamentally, just looking at the raw values, in a spreadsheet for example. While this may seem trivial, many people ignore this crucial step!
1. Compute summary statistics likes means, medians, and standard deviations.
1. Create data visualizations.

We start off by looking at the raw values. You can do this by running `View(evals)` in the console to pop-up the spreadsheet viewer. Here is a snapshot of 5 randomly chosen rows:

```{r, echo=FALSE}
evals %>%
  sample_n(5) %>%
  knitr::kable(
    digits = 3,
    caption = "Random sample of 5 instructors",
    booktabs = TRUE
  )
```

Let's also use the `glimpse()` function:

```{r}
glimpse(evals)
```

We see that both variables of interest, `bty_avg` and `score`, consist of numerical values, as well as the `age` variable. The variables `ethnicity`, `gender`, `language`, and `rank` however are categorical. Let's now compute some summary statistics for both the explanatory/predictor variable and the outcome variable using the `summary()` function, which returns the minimum, the first quartile, the median, the mean AKA the average, the third quartile, and the maximum.

```{r}
summary(evals$bty_avg)
summary(evals$score)
```

We get an idea of how the values in both variables distributed. For example, the average teaching score was 4.17 out of 5 whereas beauty scores range from 1.67 to 8.17. The `summary()` function however only returns what are called *univariate* summaries i.e. summaries about single variables. Since we are considering the relationship between two numerical variables, it would be nice to have a summary statistic that simultaneously considers both variables. The correlation coefficient is a *bivariate* summary statistic that fits this bill. It is a value between -1 and 1 that summarizes the *strength of the linear relationship between two numerical variables*; for more discussion on the correlation coefficient, see Section \@ref(correlationcoefficient) below. The correlation coefficient is computed using the `cor()` function, and in our case, the correlation is positive. 

```{r}
cor(evals$score, evals$bty_avg)
```

Since both the `score` and `bty_avg` variables are numerical, a scatterplot is an appropriate graph to visaulize this data. Let's do this using `geom_point()` and set informative axes labels and title.

```{r numxplot1, warning=FALSE, fig.cap="Instructor evaluation scores at UT Austin"}
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Teaching Score", title = "Relationship of teaching and beauty scores")
```

However Figure \@ref(fig:numxplot1) suffers from *overplotting*; let's break it up with a little random jitter added to the points in Figure \@ref(fig:numxplot2); note that we are only altering the visualization of the points; the original data stays the same.

```{r numxplot2, warning=FALSE, fig.cap="Instructor evaluation scores at UT Austin: Jittered"}
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter() +
  labs(x = "Beauty Score", y = "Teaching Score", title = "Relationship of teaching and beauty scores")
```

From this visualization we make several observations:

1. Most beauty scores lie between 2 and 8
1. Most teaching scores lie between 3 and 5
1. Most importantly, there seems to be a slight positive relationship between teaching score and beauty score, meaning as instructors have higher beauty scores, they tend to have higher teaching scores as well!

Let's first improve on Figure \@ref(fig:numxplot2) by adding the "best-fitting" line in Figure \@ref(fig:numxplot3); this is easily done by adding a new layer to the plot `+ geom_smooth(method="lm")`. 

```{r numxplot3, warning=FALSE, fig.cap="Simple linear regression line with error band"}
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Teaching Score", title = "Relationship of teaching and beauty scores") +  
  geom_smooth(method = "lm")
```

The positive slope of the blue line is consistent with our observation from earlier that there is a positive relationship between `score` and `bty_avg`. What do we mean by "best-fitting" line? While the intuition is easy to visualize in the above plot, there is a mathematically precise definition of "best", which we discuss in Section \@ref(leastsquares) below.

What are the grey bands surrounding the blue line? These are *standard error* bands, which can be thought of as error/uncertainty bands. We'll revisit this concept later so let's suppress these grey bars for now by adding the argument `se=FALSE` to `geom_smooth(method = "lm")`:

```{r numxplot4, warning=FALSE, fig.cap="Simple linear regression line without error bands"}
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Teaching Score", title = "Relationship of teaching and beauty scores") +
  geom_smooth(method = "lm", se = FALSE)
```


### Simple linear regression {#model1table}

Need to describe what simple linear regression is. 

In order to define a line like the one in Figure \@ref(fig:numxplot4), we need two quantities called *coefficients*: an intercept coefficient (the value of $y$ at $x=0$) and a slope coefficient. What are the intercept and slope coefficients of the above blue best fitting simple linear regression line? Let's get these values by outputting something called the *linear regression table* using the `get_regression_table()` function that we've included in the `moderndive` R package:

```{r, eval=FALSE}
get_regression_table(score ~ bty_avg, data = evals)
```
```{r, echo=FALSE}
evals_line <- get_regression_table(score ~ bty_avg, data = evals) %>%
  pull(estimate)
```
```{r numxplot4b, echo=FALSE}
get_regression_table(score ~ bty_avg, data = evals) %>%
  knitr::kable(
    digits = 3,
    caption = "Linear regression table",
    booktabs = TRUE
  )
```

Let's consider the inputs to this function first. This function always has form `get_regression_table(y~x, data, digits = 3, print = FALSE)`:

1. `y ~ x` is a "formula" input that tells R to fit a simple linear regression of the outcome variable `y` as a function of `x`. Above since we are interested in explaining `score` as a function of `bty_avg`, we input `score ~ bty_avg`. Be very careful to always put the outcome variable before the tilde.
1. `data` is the data frame that contains the variables `y` and `x`. In the above case, we set `data = evals`
1. `digits` specifies the digits of precision we want the regression table to have. `digits` defaults to 3, in other words, if you don't specify this argument, `digits = 3` is used by default.
1. `print` is a `TRUE`/`FALSE` boolean value that sets whether or not the output should be printed in Markdown table format, suitable for use in R markdown

Now let's consider the output regression, which has two rows denoted by the first column `term`: one corresponding to the intercept $b_0$ and one corresponding to the slope $b_{\text{bty_avg}}$ `bty_avg`. The second column `estimate` gives us the fitted values for both these values, thus in Figure \@ref(fig:numxplot4) the intercept is `r evals_line[1]`, meaning for an instructor that had a beauty score of $x=0$, they had on average a teaching score of `r evals_line[1]`. In this case however, while the intercept has a mathematical interpretation, there is no practical interpretation no instructors had anywhere near a beauty score of 0. Furthermore, since `score` is an average of a panel of 6 students' ratings from 1 to 10, a `bty_avg` of 0 would be impossible.

More interestingly is the slope associated with `bty_avg` of `r evals_line[2]`. The interpretation is: for every increase of 1 in `bty_avg`, there is an *associated* increase of *on average* `r evals_line[2]` units of `score`. We note in particular that the sign of this slope is positive, again suggestive of a positive relationship between beauty scores and teaching scores. We are very careful with our wording:

* We say there is an *associated* increase, to be cautious not to imply causation.
* We say that this associated increase is *on average* `r evals_line[2]` units of `score` because this increase isn't always exactly `r evals_line[2]` as we can see in Figure \@ref(fig:numxplot4), but rather the average associated increase.

But what about the remaining 5 columns? We'll revisit these when we cover *inference for regression* after you've covered Chapter \@ref(hypo) where you learn about *hypothesis testing* and Chapter \@ref(ci) where you'll learn about *confidence intervals*. For now, we'll only focus on the `term` and `estimate` columns.

We've included the `get_regression_table()` in the `moderndive` package to make generating clean regression tables easy, and thus have hidden a lot of details from you. If you are curious to know what is going on under the hood of this function, see Section \@ref(underthehood) below. 


### Observed/fitted values and residuals {#model1points}

We just saw how to get the value of the intercept and the slope of the regression line from the regression table generated by `get_regression_table()`. Now instead, say we want information on individual points, in this case one of the `n=463` instructors in this dataset, one corresponding to each row of `evals`. For example, say we are interested in the 21st instructor in this dataset:

```{r, echo=FALSE}
index <- which(evals$bty_avg == 7.333 & evals$score == 4.9)
target_point <- get_regression_points(score ~ bty_avg, data = evals) %>% 
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$score_hat
resid <- target_point$residual
evals %>%
  slice(index) %>%
  knitr::kable(
    digits = 3,
    caption = "Data for 21st instructor",
    booktabs = TRUE
  )
```

What is the value on the blue line corresponding to this instructors `bty_avg` of `r x`? In Figure \@ref(fig:numxplot5) we mark three values in particular corresponding to this instructor:

* The red circle corresponds to the *observed value* $y$ = `r y` and corresponds to this instructors' observed teaching score.
* The red square corresponds to the *fitted value* $\widehat{y}$ = `r y_hat` and corresponds to the value on the regression line for $x = `r x`$. We compute this value using the intercept and slope in the regression table above: $\widehat{y} = b_0 + b_1 x = `r evals_line[1]` + `r evals_line[2]` \times `r x` = `r y_hat`$
* The length of the blue arrow is the *residual* $y - \widehat{y} = `r y` - `r y_hat` = `r y-y_hat`. The residual can be thought of as the error/lack-of-fit for this particular point. So the bigger the residual, the less the fitted value $\widehat{y}$ matches up with the observed value $y$.

```{r numxplot5, echo=FALSE, warning=FALSE, fig.cap="Example of observed value, fitted value, and residual"}
best_fit_plot <- ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Teaching Score", title = "Relationship of teaching and beauty scores") + 
  geom_smooth(method = "lm", se = FALSE) +
  annotate("point", x = x, y = y, col = "red", size = 3) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 3) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))
best_fit_plot
```

What if we want both

1. the fitted value $\widehat{y} = b_0 + b_1 \times x$
1. the residual $y - \widehat{y}$

not only the 21st instructor but

* for all `r nrow(evals)` instructors in the study, in other words
* for all `r nrow(evals)` rows in the `evals` data frame, in other words
* for all `r nrow(evals)` points in regression plot in Figure \@ref(fig:numxplot4)?

We could repeat the above calculations 463 times, but that would be tedious and time consuming. Instead, let's use the `get_regression_points()` function that we've included in the `moderndive` R package. Note that in the table below we only present the results for 5 arbitrarily chosen rows out of `r nrow(evals)`.

```{r, eval=FALSE}
regression_points <- get_regression_points(score ~ bty_avg, data = evals)
regression_points
```
```{r, echo=FALSE}
set.seed(76)
regression_points <- get_regression_points(score ~ bty_avg, data = evals) 
regression_points %>%
  slice(c(index, sample(1:nrow(evals), 4))) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (5 arbitrarily chosen rows out of 463)",
    booktabs = TRUE
  )
```

Just as with the `get_regression_table()` function, the inputs to the `get_regression_points()` function are the same, however the outputs are different. Let's inspect the individual columns:

* The `score` column represents the observed value of the outcome variable $y$
* The `bty_avg` column represents the values of the explanatory/predictor variable $x$
* The `score_hat` column represents the fitted values $\widehat{y}$
* The `residual` column respresents the residuals $y - \widehat{y}$

The prove to ourselves that the results make sense, let's manually replicate the internal computation that `get_regression_points()` performs using the `mutate()` verb from Chapter \@ref(wrangling)

1. First we compute a duplicate of the fitted values $\widehat{y} = b_0 + b_1 \times x$ where the intercept $b_0$ = `r evals_line[1]` and the slope $b_1$ = `r evals_line[2]` and store these in `score_hat_2`
1. Then we compute the a duplicated the residuals $y - \widehat{y}$ and store these in `residual_2`

```{r, eval=FALSE}
regression_points <- regression_points %>% 
  mutate(score_hat_2 = 3.880 + 0.067 * bty_avg) %>% 
  mutate(residual_2 = score - score_hat_2)
regression_points
```
```{r, echo=FALSE}
set.seed(76)
regression_points <- regression_points %>% 
  mutate(score_hat_2 = 3.880 + 0.067 * bty_avg) %>% 
  mutate(residual_2 = score - score_hat_2)
regression_points %>%
  slice(c(index, sample(1:nrow(evals), 4))) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (5 arbitrarily chosen rows out of 463)",
    booktabs = TRUE
  )
```

We see that our manually computed variables `score_hat_2` and `residual_2` equal the original results (up to rounding error).

### Residual analysis {#model1resdiuals}

Recall the residuals can be thought of the error or the "lack-of-fit" between the observed value $y$ and the fitted value $\widehat{y}$ on the blue regression line in Figure \@ref(fig:numxplot4). Ideally when we fit a regression model, we'd like there to be *no systematic pattern* to these residuals. In other words, the error is seemingly random. What does this mean?

1. The residuals should be on average 0. In other words, sometimes we'll make a positive error, sometimes we'll make a negative error, but on average the error is 0. 
1. The spread of the residuals should be consistent.
1. The value of the residuals should not depend on the value of x. If it did, then the errors would not be seemingly random.

Investigating any such patterns is known as *residual analysis*. Let's perform our residual analysis in two ways. First, recall in Figure \@ref(fig:numxplot5) above we plotted:

* On the y-axis: $y$ teaching score
* On the x-axis: $x$ beauty score
* Blue arrow: one example of a residual

Instead, in Figure \@ref(fig:numxplot6) below let's plot

* On the y-axis: the residual $y-\widehat{y}$ instead
* On the x-axis: $x$ beauty score (same as before)

```{r numxplot6, echo=FALSE, warning=FALSE, fig.cap="Plot of residuals over beauty score"}
ggplot(regression_points, aes(x = bty_avg, y = residual)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue", size = 1) +
  annotate("point", x = x, y = resid, col = "red", size = 3) +
  annotate("point", x = x, y = 0, col = "red", shape = 15, size = 3) +
  annotate("segment", x = x, xend = x, y = resid, yend = 0, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))
```

You can think of Figure \@ref(fig:numxplot6) as Figure \@ref(fig:numxplot5), but
with the blue line flattened out. Does it seem like there is *no systematic pattern*
to the residuals? This question is rather qualitative and subjective in nature, thus different people may respond different. However, it can be argued that there isn't a *drastic* pattern in the residuals. 

Here are some hypothetical examples where there are *drastic* patterns to the
residuals:

```{r numxplot7, echo=FALSE, warning=FALSE, fig.cap="Examples of less than ideal residual patterns"}
resid_ex <- evals
resid_ex$ex_1 <- ((evals$bty_avg - 5) ^ 2 - 6 + rnorm(nrow(evals), 0, 0.5)) * 0.4
resid_ex$ex_2 <- (rnorm(nrow(evals), 0, 0.075 * evals$bty_avg ^ 2)) * 0.4
  
resid_ex <- resid_ex %>%
  select(bty_avg, ex_1, ex_2) %>%
  gather(type, eps, -bty_avg) %>% 
  mutate(type = ifelse(type == "ex_1", "Example 1", "Example 2"))

ggplot(resid_ex, aes(x = bty_avg, y = eps)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue", size = 1) +
  facet_wrap(~type)
```

The second way to look at the residuals is using a histogram:

```{r numxplot8, warning=FALSE, fig.cap="Histogram of residuals"}
ggplot(regression_points, aes(x = residual)) +
  geom_histogram(binwidth = 0.25, color = "white") +
  labs(x = "Residual")
```

This histogram seems to indicate that we have more positive residuals than negative. Since residual = $y-\widehat{y} > 0$ when $y > \widehat{y}$, it seems our fitted teaching score from the regression model tend to *underestimate* the true teaching score. This histogram has a slight *left-skew* in that there is a long tail on the left. Another way to say this is this data exhibit a *negative skew*. Here are examples of an ideal and less than ideal patterns of residuals.

```{r numxplot9, echo=FALSE, warning=FALSE, fig.cap="Examples of ideal and less than ideal residual patterns"}
resid_ex <- evals
resid_ex$`Ideal` <- rnorm(nrow(resid_ex), 0, sd = sd(regression_points$residual))
resid_ex$`Less than ideal` <-
  rnorm(nrow(resid_ex), 0, sd = sd(regression_points$residual))^2

resid_ex <- resid_ex %>%
  select(bty_avg, `Ideal`, `Less than ideal`) %>%
  gather(type, eps, -bty_avg)

ggplot(resid_ex, aes(x = eps)) +
  geom_histogram(binwidth = 0.25, color = "white") +
  labs(x = "Residual") +
  facet_wrap( ~ type, scales = "free")
```

In fact, we'll see later on that we would like the residuals to be *normally distributed* with
mean 0. In other words, be bell-shaped and centered at 0! While this requirement and residual analysis in general may some of you as not being overly critical at this point, we'll see later after we've covered *statistical inference* that for the last five columns of the regression table from earlier (std error, statistic, p-value, conf low, and conf high) to have valid interpretations, the above three conditions should roughly hold. More on this in Section . 

### Your Turn

Repeat the above analysis where teaching `score` is the outcome variable, but now use `age` as the explanatory/predictor variable. Recall the steps:

1. Perform an exploratory data analysis by
    a) Looking at the raw values
    a) Computing summary statistics of the variables of interest
    a) Creating informative visualizations
1. Fit a linear regression model and get information about the "best-fitting" line from the regression table by using the `get_regression_table()` function
1. Get information on all $n$ data points AKA all $n$ rows in the dataset AKA all instructors, in particular the fitted values and the residuals, using the `get_regression_points()` function.
1. Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern.


 






## 5RM#2: Life Expectancy {#model2}

It's an unfortunate truth that life expectancy is not the same across various countries in the world; there are a multitude of factors that are associated with how long people live. International development agencies are very interested in studying these differences in the hope of understanding where governments should allocate ressources to address this problem. One way to compare differences in life expectancy is between continents: "Do countries in certain continents have higher life expectancy?" or "Do certain continents have a lot of variation in life expectancy?"

To this end, let's study the `gapminder` data frame in the `gapminder` package. Recall we introduced this dataset in Chapter \@ref(gapminder) when we first studied the "Grammar of Graphics"; in particular Figure \@ref(fig:gapminder). This dataset has development data about various countries for 5-year intervals between 1952 and 2007. Let's have

* Outcome variable $y$: Mean life expectancy in 2007 for $n=142$ countries
* Explanatory variables $x$
    + continent
    + GDP per capita

Let’s load the `gapminder` data, `filter()` for only observations in 2007, `select()` only a subset of the variables, and save this in a data frame `gapminder2007`.

```{r, warning=FALSE, message=FALSE}
library(gapminder)
gapminder2007 <- gapminder %>%
  filter(year == 2007) %>% 
  select(country, continent, lifeExp, gdpPercap)
```

In this section we'll try to explain between country differences in life expectancy as a function of which continent the country is located in. We'll model thevrelationship between these two variables with linear regression again, but note that our explanatory/predictor variable $x$ is now categorical, and not numerical like when we covered simple linear regression in Section \@ref(regressionmodel1):

1. A numerical outcome variable $y$, in this case `lifeExp`
1. A single numerical explanatory/predictor variable $x$, in this case `continent`

The concept of a "best-fitting" line is a little different when the explanatory/predictor variable $x$ is numerical; we'll study these differences shortly.


### Exploratory data analysis {#model2EDA}

Let's look at the raw data values. Either by bringing up RStudio's spreadsheet viewer. Here is a snapshot of 5 randomly chosen rows out of the `r nrow(gapminder2007)` countries

```{r, eval=FALSE}
View(gapminder2007)
```

```{r, echo=FALSE}
gapminder2007 %>%
  sample_n(5) %>%
  knitr::kable(
    digits = 3,
    caption = "Random sample of 5 countries",
    booktabs = TRUE
  )
```

or using the `glimpse()` function.

```{r}
glimpse(gapminder2007)
```

We see that the variable `continent` is indeed categorical, as it is encoded as `fctr` which stands for "factor": R's way of storing categorical variables. Let's look at a summary of the explanatory/predictor variable `continent`:

```{r}
summary(gapminder2007$continent)
```

We observe that all continents have 25 countries or more, but Oceania only has two: Australia and New Zealand. Let's now compute some summary statistics of the outcome variable `lifeExp`, in particular the worldwide median and mean life expectancy

```{r, eval=TRUE}
lifeExp_worldwide <- gapminder2007 %>%
  summarize(median = median(lifeExp), mean = mean(lifeExp))
```
```{r, echo=FALSE}
lifeExp_worldwide %>%
  knitr::kable(
    digits = 3,
    caption = "Worldwide life expectancy",
    booktabs = TRUE
  )
```

Worldwide roughly half the countries had life expectancies of `r lifeExp_worldwide$median %>% round(3)` years or less, while roughly half were higher. The mean however is lower at `r lifeExp_worldwide$mean %>% round(3)`. Why are these two values different? Let's look at a histogram of `lifeExp` to see why.

```{r}
ggplot(gapminder2007, aes(x = lifeExp)) +
  geom_histogram(binwidth = 5, color = "white") +
  labs(x = "Life expectancy", y = "Number of countries", title = "Worldwide life expectancy")
```

We say that this data is *left-skewed*: there are a few countries with very low life expectancies that are brining down the mean life expectancy. However, the median is less sensitive to the effects of such outliers. Hence the median is greater than the mean in this case. Let's proceed by comparing median and mean life expectancy between continents by adding a `group_by(continent)` to the above code:

```{r, eval=TRUE}
lifeExp_by_continent <- gapminder2007 %>%
  group_by(continent) %>%
  summarize(median = median(lifeExp), mean = mean(lifeExp))
```
```{r catxplot0, echo=FALSE}
lifeExp_by_continent %>%
  knitr::kable(
    digits = 3,
    caption = "Life expectancy by continent",
    booktabs = TRUE
  )
```
```{r, echo=FALSE}
median_africa <- lifeExp_by_continent %>%
  filter(continent == "Africa") %>%
  pull(median)
mean_africa <- lifeExp_by_continent %>%
  filter(continent == "Africa") %>%
  pull(mean)
n_countries <- gapminder2007 %>% nrow()
n_countries_africa <- gapminder2007 %>% filter(continent == "Africa") %>% nrow()
```

We now that there are differences in life expectancies between the continents. For example, while the median life expectancy across all $n = `r n_countries`$ countries in 2007 was `r lifeExp_worldwide$median %>% round(3)`, the median life expectancy across only the $n =`r n_countries_africa `$ countries in Africa was only `r median_africa`.

Let's create an appropriate visualization. One way to compare the life expectancies of countries in different continents would be via a facetted histogram:

```{r catxplot0b, warning=FALSE, fig.cap="Life expectancy in 2007"}
ggplot(gapminder2007, aes(x = lifeExp)) +
  geom_histogram(binwidth = 5, color = "white") +
  labs(x = "Life expectancy", y = "Number of countries", title = "Life expectancy by continent") +
  facet_wrap(~continent, nrow = 2)
```

Another way would be via a boxplot, which is well suited for visualizing one numerical and one categorical variable:

```{r catxplot1, warning=FALSE, fig.cap="Life expectancy in 2007"}
ggplot(gapminder2007, aes(x = continent, y = lifeExp)) +
  geom_boxplot() +
  labs(x = "Continent", y = "Life expectancy (years)", title = "Life expectancy by continent") 
```

We can now easily compare life expectancies between continents with single horizontal lines. Something that people new to boxplots forget is that the solid bars in the middle of the boxes correspond to **medians and not means**. Let's modify this plot in two ways. First, let's treat the median life expectancy years for Africa of `r median_africa` as a *baseline for comparison* are mark this value on the y-axis with a horizontal line via `geom_hline()`:

```{r catxplot1b, warning=FALSE, fig.cap="Life expectancy in 2007"}
ggplot(gapminder2007, aes(x = continent, y = lifeExp)) +
  geom_boxplot() +
  labs(x = "Continent", y = "Life expectancy (years)", title = "Life expectancy by continent") +
  geom_hline(yintercept = 52.93, color = "red")
```

Second, let's recenter the y-axis at Africa's median life expectancy by subtracting `r median_africa` from all values. This allows us to focus on relative differences from Africa's median life expectancy.

```{r catxplot2, warning=FALSE, fig.cap="Difference in life expectancy relative to African median of 52.93 years"}
ggplot(gapminder2007, aes(x = continent, y = lifeExp - 52.93)) +
  geom_boxplot() +
  geom_hline(yintercept = 52.93 - 52.93, col = "red") +
  labs(x = "Continent", y = "Difference in life expectancy vs Africa (years)",
       title = "Life expectancy relative to Africa")
```

Using the "eyeball test", we make the following observations:

* Differences in median life expectancy vs. the baseline for comparison, Africa's median life expectancy of `r median_africa` years:
    1. The median life expectancy of the Americas is roughly 20 years greater.
    1. The median life expectancy of Asia is roughly 20 years greater.
    1. The median life expectancy of Europe is roughly 25 years greater.
    1. The median life expectancy of Oceania is roughly 27.8 years greater.
* Africa and Asia have much more spread/variation in life expectancy as indicated by the interquartile range (the height of the boxes).
* Oceania has almost no spread/variation, but this might in large part be due to the fact there are only two countries in Oceania. 
 

### Linear regression {#model2table}

In Section \@ref(model1table) we introduced *simple* linear regression, which involves modeling the relationship between a numerical outcome variable $y$ as a function of a numerical explanatory/predictor variable $x$, in our life expectancy example, we now have a categorical explanatory/predictor variable $x$ `continent`. While we still can fit a regression model, given our categorical explanatory/predictor variable we no longer have a concept of a "best-fitting" line, but differences relative to a baseline for comparison.

Before we fit our regression model, let's create a table similar to Table \@ref(tab:catxplot0), but

1. Report the mean life expectancy for each continent.
1. Report the difference in mean life expectancy *relative* to Africa's mean life expectancy of `r mean_africa` in the column "mean vs Africa"; this column is simple the "mean" column minus `r mean_africa`.

Think back to your observations from the eye-ball test of Figure \@ref(fig:catxplot2) at the end of the last section. The column "mean vs Africa" is the same idea of comparing a summary statistic to a baseline for comparison, in this case the countries of Africa, but using means instead of medians. 

```{r continent-mean-life-expectancies, echo=FALSE}
gapminder2007 %>%
  group_by(continent) %>%
  summarize(mean = mean(lifeExp)) %>%
  mutate(`mean vs Africa` = mean - mean_africa) %>% 
  knitr::kable(
    digits = 3,
    caption = "Mean life expectancy by continent",
    booktabs = TRUE
  )
```

Now, let's use the `get_regression_table()` function we introduced in Section \@ref(model1table) to obtain the *regression table* for `gapminder2007` analysis: 

```{r, eval=FALSE}
get_regression_table(lifeExp ~ continent, data = gapminder2007)
```
```{r, echo=FALSE}
evals_line <- get_regression_table(lifeExp ~ continent, data = gapminder2007) %>%
  pull(estimate)
```
```{r catxplot4b, echo=FALSE}
get_regression_table(lifeExp ~ continent, data = gapminder2007) %>%
  knitr::kable(
    digits = 3,
    caption = "Linear regression table",
    booktabs = TRUE
  )
```

Just as before, we have the `term` and `estimates` columns of interest, but unlike before, we now have 5 rows corresponding to 5 outputs in our table: an intercept like before, but also `continentAmericas`, `continentAsia`, `continentEurope`, and `continentOceania`. What are these values?

1. `intercept = 54.8` corresponds to the mean life expectancy for Africa. This mean life expectancy is treated as a baseline for comparison for the other continents.
1. `continentAmericas = 18.8` is the difference in mean life expectancies of the
Americas minus Africa. Note that 18.80 = 73.6 - 54.8 is the 2nd "mean vs Africa"
value in Table \@ref(tab:continent-mean-life-expectancies).
1. `continentAmericas = 15.9` is the difference in mean life expectancy of Asia 
minus Africa. Note that 15.9 = 70.7 - 54.8 is the 2nd "mean vs Africa"
value in Table \@ref(tab:continent-mean-life-expectancies).
1. `continentEurope = 22.8` is the difference in mean life expectancy of Europe 
minus Africa. Note that 22.8 = 77.6 - 54.8 is the 3rd "mean vs Africa"
value in Table \@ref(tab:continent-mean-life-expectancies).
1. `continentOceania = 25.9` is the difference in mean life expectancy of Oceania
minus Africa. Note that 25.9 = 80.7 - 54.8 is the 3rd "mean vs Africa"
value in Table \@ref(tab:continent-mean-life-expectancies).

Let's generalize this idea a bit. If we fit a linear regression model using an explanatory/predictor variable $x$ that has $k$ levels, a regression model will return an intercept and $k-1$ "slope" coefficients of differences in means relative to a baseline mean for comparison. In our case, since there are $k=5$ continents, the regression model returns an intercept corresponding to the baseline for comparison Africa and $k-1=4$ slope coefficients corresponding to the Americas, Asia, Europe, and Oceania. Africa was chosen as the baseline by R for no other reason than it is first alphabetically of the 5 continents. Note you can manually specify which continent to use as baseline, but we leave that to a more advanced course.


### Observed/fitted values and residuals {#model2residuals}

What do fitted values $\widehat{y}$ and residuals $y - \widehat{y}$ correspond to when the explanatory/predictor variable $x$ is categorical? Let's investigate these values for the first 10 countries in the `gapminder2007` dataset: 

```{r, echo=FALSE}
gapminder2007 %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "First 10 out of 142 countries",
    booktabs = TRUE
  )
```

Recall the `get_regression_points()` function we used in Section \@ref(model1points) that returns the fitted value and residual for 

* for all `r nrow(gapminder2007)` countries in our dataset
* for all `r nrow(gapminder2007)` rows in the `gapminder2007` data frame
* for all `r nrow(gapminder2007)` data points used in the five boxplots in Figure \@ref(fig:catxplot2) 

```{r, eval=FALSE}
regression_points <- get_regression_points(lifeExp ~ continent, data = gapminder2007)
regression_points
```
```{r, echo=FALSE}
regression_points <- get_regression_points(lifeExp ~ continent, data = gapminder2007)
regression_points %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (First 10 out of 142 countries)",
    booktabs = TRUE
  )
```

Notice

* The fitted values $\widehat{\text{lifeexp}}$. Countries in Africa have the
same fitted value of 54.8, which is the mean life expectancy of Africa;
countries in Asia have the same fitted value of 70.7, which is the mean life
expectancy of Asia; this similarly holds for countries in the Americas, Europe,
and Oceania.
* The `residual` column is simply $y - \widehat{y}$ = `lifeexp - lifeexp_hat`.
These values can be interpreted as that particular countries deviation from the
mean life expectancy of the respective continent's mean. For example, the first
row of this dataset corresponds to Afghanistan, and the residual of -26.9 = 43.8
- 70.7 is Afghanistan's mean life expectancy minus the mean life expectancy of
all Asian countries.


### Residual analysis {#model2residuals}

Recall our discussion on residuals from Section \@ref(model1residuals) where our goal was to investigate whether or not there was a *systematic pattern* to the residuals, as ideally since residuals can be thought of as error, there should be no such pattern. While there are many ways to do such residual analysis, we focused on two approaches based on visualizations. 

1. A plot with residuals on the y-axis and the predictor (in this case continent) on the x-axis
1. A histogram of all residuals

First, let's plot the residuals vs continent in Figure \@ref(fig:catxplot7), but also let's plot all `r nrow(gapminder2007)` points with a little horizontal random jitter by setting the `width = 0.1` parameter in `geom_jitter()` 

```{r catxplot7, warning=FALSE, fig.cap="Plot of residuals over continent"}
ggplot(regression_points, aes(x = continent, y = residual)) +
  geom_jitter(width = 0.5) + 
  labs(x = "Continent", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue")
```

We observe:

1. While not perfectly balanced above and below the line $y=0$, there seems to
be a rough balance of both positive and negative residuals for all 5 continents.
1. However, there is one clear outlier in Asia. It has the smallest residual,
hence also has the smallest life expectancy in Asia.

Let's see investigate the 5 countries in Asia with the shortest life expectancy:

```{r, eval=FALSE}
gapminder2007 %>%
  filter(continent == "Asia") %>%
  arrange(lifeExp)
```
```{r, echo=FALSE}
gapminder2007 %>%
  filter(continent == "Asia") %>%
  arrange(lifeExp) %>%
  slice(1:5) %>%
  knitr::kable(
    digits = 3,
    caption = "Countries in Asia with shortest life expectancy",
    booktabs = TRUE
  )
```

This was the earlier identified residual for Afghanistan of -26.9. Unfortunately 
given recent geopolitical turmoil, individuals who live in Afghanistan have a 
drastically lower life expectancy. 

Second, let's look at a histogram of all `r nrow(gapminder2007)` values of
residuals in Figure \@ref(fig:catxplot8). In this case, the residuals form a
rather nice bell-shape, although there are a couple of very low and very high
values at the tails. As we said previously, searching for patterns in residuals
can be somewhat subjective, but ideally we hope there are no "drastic" patterns.

```{r catxplot8, warning=FALSE, fig.cap="Histogram of residuals"}
ggplot(regression_points, aes(x = residual)) +
  geom_histogram(binwidth = 5, color = "white") +
  labs(x = "Residual")
```


### Your turn

Repeat this same analysis but using GDP per capita `gdpPercap` as the outcome variable. Recall the four steps we've been following:

1. Perform an exploratory data analysis
1. Fit a linear regression model and get information about the model
1. Get information on all $n$ data points considered in this analysis. 
1. Perform a residual analysis and look for any systematic patterns in the residuals.





## 5RM#3: Credit Card Balance {#model3}

Much like our teacher evaluation data in Section \@ref(regressionmodel1), let's now attempt to identify factors that are associated with how much credit card debt an individual will hold. The textbook [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani is an intermediate-level textbook on statistical and machine learning freely available [here](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf). It has an accompanying R packaged called `ISLR` with datasets that the authors use to demonstrate various machine learning methods. 
One dataset that is frequently used by the authors is the `Credit` dataset where predictions are made on the balance held by various credit card holders based on information like income, credit limit, and education level. Let's consider the following variables:

* Outcome variable $y$: Credit card balance in dollars
* Explanatory variables $x$:
    + income in $1000's
    + credit card limit in dollars
    + credit rating
    + card holder's age
    + years of education

Let’s load the data and `select()` only the 6 variables listed above:

```{r}
library(ISLR)
Credit <- Credit %>%
  select(Balance, Income, Limit, Rating, Age, Education)
```

Unfortunately, no information on how this data was sampled was provided, so we're not able to generalize any such results to a greater population, but we will still use this data to demonstrate *multiple regression*. Multiple regression is a form of linear regression where there are now more than one explanatory/predictor variables and thus the interpretation of the associated effect of any one explanatory/predictor variable must be made in conjuction with the other explanatory/predictor variable. We'll perform multiple regression with:

1. A numerical outcome variable $y$, in this case teaching credit card `Balance`
1. Two explanatory/predictor variables:
    1. A first numerical explanatory/predictor variable $x_1$, in this case credit `Limit`
    1. A second numerical explanatory/predictor variable $x_2$, in this case `Income`


### Exploratory data analysis {#model3EDA}

First, let’s look at a random sample of 5 out of the 400 card holders:

```{r, echo=FALSE}
Credit %>%
  sample_n(5) %>%
  knitr::kable(
    digits = 3,
    caption = "Random sample of 5 credit card holders",
    booktabs = TRUE
  )
```


Let's load the Credit card balance `Credit` data frame
that we created in the "Datasets" Section \@ref(datacreditcard).

Now that we have two numerical predictors, an ideal way to visualize this data
is with a 3D scatterplot. Unfortunately this is beyond the scope of this book,
so let's do the next best thing and look at two separate scatterplots of the outcome
variable $y$ credit card balance in dollars with



```{r 2numxplot1, warning=FALSE, fig.cap="Relationship between credit card balance and credit limit"}
ggplot(Credit, aes(x = Limit, y = Balance)) +
  geom_point() +
  labs(x = "Credit limit (in $)", y = "Credit card balance (in $)") +
  geom_smooth(method = "lm", se = FALSE)
```

```{r 2numxplot2, warning=FALSE, fig.cap="Relationship between credit card balance and income"}
ggplot(Credit, aes(x = Income, y = Balance)) +
  geom_point() +
  labs(x = "Income (in $1000)", y = "Credit card balance (in $)") +
  geom_smooth(method = "lm", se = FALSE)
```

In both cases, there seems to be a positive relationship. Let's look at the 
correlation coefficient in both cases. 

```{r, eval=FALSE}
cor(Credit$Limit, Credit$Balance)
```
```{r, echo=FALSE}
Credit %>%
  summarize(correlation = cor(Limit, Balance)) %>% 
  knitr::kable(
    digits = 3,
    caption = "Correlation between credit limit and credit card balance", 
    booktabs = TRUE
  )
```


```{r, eval=FALSE}
cor(Credit$Income, Credit$Balance)
```
```{r, echo=FALSE}
Credit %>% 
  summarize(correlation = cor(Income, Balance)) %>% 
  knitr::kable(
    digits = 3,
    caption = "Correlation between income (in $1000) and credit card balance", 
    booktabs = TRUE
  )
```

In both cases, the correlation coefficient indicates a strong linear relationship,
in particular between credit limit and credit card balance! Makes sense! Note 
however there were a few individuals who had balances of 0!

What if in instead we looked the correlation coefficient between income and
credit card balance, where income was in dollars and not thousands of dollars? i.e. we 
multiply Income by 1000?

```{r, eval=FALSE}
cor(Credit$Income * 1000, Credit$Balance)
```
```{r, echo=FALSE}
Credit %>%
  summarize(correlation = cor(Income, Balance)) %>%
  knitr::kable(
    digits = 3,
    caption = "Correlation between income (in $) and credit card balance", 
    booktabs = TRUE
  )
```

We see it is the same! We say that correlation coefficient is invariant to linear
transformations! In other words

* The correlation between $x$ and $y$ will be the same as
* The correlation between $a\times x + b$ and $y$ where $a, b$ are numerical values (real numbers in mathematical terms)


### Multiple regression {#model3table}

Let's now compute the *regression table*

```{r, eval=FALSE}
get_regression_table(Balance ~ Limit + Income, data = Credit)
```
```{r, echo=FALSE}
get_regression_table(Balance ~ Limit + Income, data = Credit) %>% 
  knitr::kable(
    digits = 3,
    caption = "Regression table", 
    booktabs = TRUE
  )
```

Whereas in simple linear regression with a single numerical predictor, for
multiple linear regression with two numerical predictors, the regression line
is now a *regression plane*. In other words, a flat surface! See the
interactive 3D scatterplot with a regression plane added [here](https://rudeboybert.github.io/STAT135/index.html#36_multiple_regression).

How do we interpret the three values that define the plane?

* Intercept: -$385.18. In this case, the intercept doesn't have a practical
interpretation as no one can have a credit card balance of less than $0. It is
used merely to situation the plane.
* Limit: $0.26. Now that we have multiple variables to consider, we have to add
a caveat to our interpretation: *all other things being equal, for every increase in one dollar in credit limit, there is an associated increase of on average 26 cents in credit card balance*. Note:
    + Again, we are not making any causal statements, only statements relating to the association between credit limit and balance
    + The *all other things being equal* related to the other explanatory/predictor variables, in this case only one: Income. This is equivalent to saying "holding income constant, we observed an associated increase of $0.26 in credit card balance for every dollar increase in credit limit"
* Income: -$7.66. Negative? Say what? Didn't we see in Figure
\@ref(fig:2numxplot2) that the relationship was positive and that the
correlation between Income and credit card balance was positive?

Here is a histogram of all `nrow(Credit)` values of `Limit` where the vertical red
lines cut up the data into quartiles.

```{r, 2numxplot3, echo=FALSE, fig.cap="Histogram of credit limits and quartiles"}
ggplot(Credit, aes(x = Limit)) +
  geom_histogram(color = "white") +
  geom_vline(xintercept = quantile(Credit$Limit, probs = c(0.25, 0.5, 0.75)), col = "red")
```

In other words, 

1. 25% of people had credit limits between \$0 and \$3088. Let's call this group "low" credit limit bracket.
1. 25% of people had credit limits between \$3088 and \$4622. Let's call this group "medium-low" credit limit bracket.
1. 25% of people had credit limits between \$4622 and \$5873. Let's call this group "medium-high" credit limit bracket.
1. 25% of peopel had credit limits over \$5873. Let's call this group "high" credit limit bracket.

```{r, 2numxplot4, echo=FALSE, warning=FALSE, fig.cap="Relationship between credit card balance and income for different credit limit brackets"}
Credit <- Credit %>% 
  mutate(limit_bracket = cut_number(Limit, 4)) %>% 
  mutate(limit_bracket = fct_recode(limit_bracket,
    "low" =  "[855,3.09e+03]",
    "medium-low" = "(3.09e+03,4.62e+03]", 
    "medium-high" = "(4.62e+03,5.87e+03]", 
    "high" = "(5.87e+03,1.39e+04]"
  ))

ggplot(Credit, aes(x = Income, y = Balance, col = limit_bracket)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Income (in $1000)", y = "Credit card balance (in $)", 
       color = "Credit limit\nbracket")
```

Figure \@ref(fig:2numxplot4) is actually the same as Figure \@ref(fig:2numxplot2), but now with colors distinguishing the credit limit brackets. We now see that for the 

1. low
1. medium-low
1. medium-high

income bracket groups, the strong positive relationship between credit card balance
and income disappears! Only for the high bracket does the relationship stay somewhat positive. In this example credit limit is a *confounding variable* for credit card balance and income.

Alternatively, we could also have used facets, where each facet has roughly 25% of people based
on the credit limit bracket. However, IMO the above plot is easier to read.

```{r, 2numxplot5, echo=FALSE, warning=FALSE, fig.cap="Relationship between credit card balance and income for different credit limit brackets"}
ggplot(Credit, aes(x = Income, y = Balance)) +
  geom_point() +
  facet_wrap(~limit_bracket) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Income (in $1000)", y = "Credit card balance (in $)")
```



 





### Observed/fitted values and residuals {#model3points}
### Residual analysis {#model3points}

## 5RM#4: Teacher Evaluations Part II {#model4}

Let's revisit the professor evaluation data introduced in Section \@ref(profevals). In
Section \@ref(#onenumerical) we studied the relationship between 

* $y$: outcome variable of instructor evaluation score
* $x$: numerical explanatory variable of beauty score

Now let's say we want to also consider age instead of beauty score and a
categorical explanatory/predictor variable: the binary gender of the professor.
Our modeling situation now becomes

* y: outcome variable of instructor evaluation score
* predictor variables
    + $x_1$: numerical explanatory/predictor variable of age
    + $x_2$: categorical explanatory/predictor variable of gender


### Exploratory data analysis {#model4EDA}

```{r numxcatxplot1, warning=FALSE, fig.cap="Instructor evaluation scores at UT Austin by gender: Jittered"}
ggplot(evals, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_smooth(method = "lm", se = FALSE)
```

We notice some interesting trends:

1. There are almost no women faculty over the age of 60.
1. Fitting separate regression lines for men and women, we see they have different slopes.
We see that the associated effect of increasing age seems to be much harsher for women than men. In other words, they have different slopes.



### Multiple regression {#model4table}

Let's now compute the *regression table*

```{r, eval=FALSE}
get_regression_table(score ~ age + gender, data = evals)
```
```{r, echo=FALSE}
get_regression_table(score ~ age + gender, data = evals) %>% 
  knitr::kable(
    digits = 3,
    caption = "Regression table", 
    booktabs = TRUE
  )
```

The modeling equation for this is:

$$
\begin{align}
\widehat{y} &= b_0 + b_1 x_1 + b_2 x_2 \\
\widehat{score} &= b_0 + b_{age} age + b_{male} \mathbb{1}[\mbox{is male}] \\
\end{align}
$$

What this looks like is in Figure \@ref(fig:numxcatxplot2) below. 

```{r numxcatxplot2, echo=FALSE, warning=FALSE, fig.cap="Instructor evaluation scores at UT Austin by gender: same slope"}
coeff <- lm(score ~ age + gender, data = evals) %>% 
  coef() %>%
  as.numeric()
slopes <- evals %>%
  group_by(gender) %>%
  summarise(min = min(age), max = max(age)) %>%
  mutate(intercept = coeff[1]) %>%
  mutate(intercept = ifelse(gender == "male", intercept + coeff[3], intercept)) %>%
  gather(point, age, -c(gender, intercept)) %>%
  mutate(y_hat = intercept + age * coeff[2])

ggplot(evals, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_line(data = slopes, aes(y = y_hat), size = 1)
```

We see that:

* Females are treated as the baseline for comparison for no other reason than "female" is alphabetically earlier than "male". The $b_{male} = 0.1906$ is the vertical "bump" that men get in their teaching evaluation scores. Or more precisely, it is the average difference in teaching score
that men get *relative to the baseline of women*
* Accordingly, the intercepts are (which in this case make no sense since no professor can have age 0):
    + for women: $b_0$ = 4.484
    + for men: $b_0 + b_{male}$ = 4.484 + 0.191 = 4.675
* Both men and women have the same slope. In other words, *in this model* the associated effect of age is the same for men and women: all other things being equal, for every increase in 1 in age,
there is on average an associated decrease of $b_{age}$ = -0.0086 in teaching score

**Hold up**: Figure \@ref(fig:numxcatxplot2) is different than Figure \@ref(fig:numxcatxplot1)! What is going on? What we have in the original plot is an *interaction effect* between age and gender!


### Mutiple regression with interaction effects
### Observed/fitted values and residuals {#model4points}


We say a model has an interaction effect if the associated effect of one variable *depends on the value of another variable*.

Let's now compute the *regression table*

```{r, eval=FALSE}
get_regression_table(score ~ age * gender, data = evals)
```
```{r, echo=FALSE}
get_regression_table(score ~ age * gender, data = evals) %>% 
  knitr::kable(
    digits = 3,
    caption = "Regression table", 
    booktabs = TRUE
  )
```

The model formula is

$$
\begin{align}
\widehat{y} &= b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1x_2\\
\widehat{score} &= b_0 + b_{age} age + b_{male} \mathbb{1}[\mbox{is male}] + b_{age,male}age\mathbb{1}[\mbox{is male}] \\
\end{align}
$$



 





### Residual analysis {#model4residuals}
## 5RM#5: Biographical Movies {#model5}

Let's look at the `biopics` dataset in the
[`fivethirtyeight`](https://rudeboybert.github.io/fivethirtyeight/) package.
After loading the package, run `?biopics` in the console to read the help file.
This data is from the article ["Straight Outta Compton" Is The Rare Biopic Not About White Dudes](https://fivethirtyeight.com/features/straight-outta-compton-is-the-rare-biopic-not-about-white-dudes/).  

First let's load the data and look at a random sample of 5 rows:

```{r}
library(fivethirtyeight)
biopics <- biopics %>%
  select(title, box_office, person_of_color, subject_sex) %>%
  # Remove those that are missing
  filter(!is.na(box_office))
```

```{r, echo=FALSE}
biopics %>%
  sample_n(5) %>%
  knitr::kable(digits = 1)
```




Before we conduct an exploratory data analysis of the `biopics` data, let's first
have a discussion $\log$-transformations.

### log-transformations

Let's consider a histogram of the box office revenues for the `biopics` dataset

```{r logtransform1, echo=TRUE, message=FALSE, warning=FALSE, fig.cap="Histogram of box office revenue"}
ggplot(biopics, aes(x = box_office)) +
  geom_histogram(color = "white") +
  labs(x = "Box office revenue")
```

In Figure \@ref(fig:logtransform1), we see there is a right-skew to both the
x-values. This is because there are a few Hollywood blockbusters being compared
with many (likely) smaller-scale independent films. Let's look at the top 5 and
bottom 5 grossing movies in this dataset: 

```{r, echo=FALSE}
biopics %>%
  arrange(desc(box_office)) %>%
  slice(1:5) %>% 
  knitr::kable(
    digits = 3,
    caption = "Top 5 grossing movies in data", 
    booktabs = TRUE
  )
```

```{r, echo=FALSE}
biopics %>%
  arrange(box_office) %>%
  slice(1:5) %>% 
  knitr::kable(
    digits = 3,
    caption = "Bottom 5 grossing movies in data", 
    booktabs = TRUE
  )
```

The scale of box office revenue is completely different! Hence, in Figure \@ref(fig:logtransform1), it's really hard to see what's going on at the
lower-end. 

Let's unskew this variable and compare not *absolute* differences, but rather,
*relative* differences i.e. differences in "order of magnitude" using a
`log10()` transformation:

```{r logtransform2, echo=TRUE, message=FALSE, warning=FALSE, fig.cap="Histogram of log10(box office revenue)"}
ggplot(biopics, aes(x = log10(box_office))) +
  geom_histogram(color = "white") +
  labs(x = "log10(Box office revenue)")
```

We can see a little better what's going on at the lower end of the box office revenue scale.
However the values on the axes require a little thinking to process. For example
at $x=7$, this corresponds to movies with revenue of $10^7 = 10,000,000$
dollars. So instead, let's *rescale* the x-axis so that it displays the data in
their original units.

```{r logtransform3, echo=TRUE, message=FALSE, warning=FALSE, fig.cap="Histogram of box office revenue (log-10 scale)"}
ggplot(biopics, aes(x = box_office)) +
  geom_histogram(color = "white") +
  scale_x_log10() +
  labs(x = "Box office revenue (log10-scale))")
```

Note that

* The two plots are identical, but the values on the x-axis are different.
* In both Figure \@ref(fig:logtransform2) Figure \@ref(fig:logtransform3), equivalent distances on each axes correspond to not equivalent absolute differences, but equivalent relative/multiplicative differences. So for example, the horizontal distance on the plot from Budget = `1e+05` = $10^5$ to Budget  = `1e+06` = $10^6$ is equal to the horizontal distance on the plot from Budget = `1e+06` = $10^6$ to Budget = `1e+07` = $10^7$.



### Exploratory data analysis {#model5EDA}

Let's now consider the box office gross earnings in the US of these movies on a log10
scale:

```{r 2catxplot, echo=TRUE, warning=FALSE, fig.cap="Box office revenue vs biopic subject info"}
ggplot(biopics, aes(x = subject_sex, y = box_office)) +
  facet_wrap(~person_of_color, nrow = 1) +
  scale_y_log10() +
  geom_boxplot() +
  labs(x = "Subject sex", y = "Box office revenue (log10-scale)", title =
  "Person of color?")
```

It seems in this dataset, men of color had the highest median box office gross. Let's
look at a table of means instead of medians.

```{r, eval=FALSE}
biopics %>%
  group_by(person_of_color, subject_sex) %>%
  summarise(mean_box_office = mean(box_office)) %>%
  arrange(desc(mean_box_office))
```
```{r, echo=FALSE}
biopics %>%
  group_by(person_of_color, subject_sex) %>%
  summarise(mean_box_office = mean(box_office)) %>%
  arrange(desc(mean_box_office)) %>% 
  knitr::kable(
    digits = 3,
    caption = "Group means", 
    booktabs = TRUE
  )
```

Keep in mind two things though. First, the sample sizes are different:

```{r, eval=FALSE}
biopics %>%
  group_by(person_of_color, subject_sex) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
```
```{r, echo=FALSE}
biopics %>%
  group_by(person_of_color, subject_sex) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>% 
  knitr::kable(
    digits = 3,
    caption = "Number of movies of each type in dataset", 
    booktabs = TRUE
  )
```

Second, can we *generalize* these results to *all movies*? How was the selection
of movies sampled? Is it a representative sample of all movies, or was their a
systematic reason these were included?



### Multiple regression {#model5table}


Let's now compute the *regression table*. Let's jump straight into considering
a model that incorporates an interaction term as described earlier. 

```{r, eval=FALSE}
get_regression_table(box_office ~ person_of_color * subject_sex, data = biopics)
```
```{r, echo=FALSE}
get_regression_table(box_office ~ person_of_color * subject_sex, data = biopics) %>% 
  knitr::kable(
    digits = 3,
    caption = "Regression table", 
    booktabs = TRUE
  )
```

The model formula is

$$
\begin{align}
\widehat{y} &= b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1x_2\\
\widehat{score} &= b_0 + b_{color}\mathbb{1}[\mbox{of color}] + b_{male} \mathbb{1}[\mbox{is male}] + b_{color,male}\mathbb{1}[\mbox{of color}]\mathbb{1}[\mbox{is male}] \\
\end{align}
$$


Recreate four group means from the Table 6.15 above:

* Female not of color: 18088799 = 18088799 = $b_0$. Note: *Women not of color are the baseline group*.
* Male not of color: 22871074 = 18088799 + 4782275 = $b_0 + b_{male}$
* Female of color: 20035820 = 18088799 + 1947021 = $b_0 + b_{color}$
* Male of color: 31648028 = 18088799 + 1947021 + 4782275 + 6829933 = $b_0 + b_{color} + b_{male} + b_{color,male}$. Note: $b_{color,male}$ is the *interaction term*.






### Observed/fitted values and residuals {#model5points}
### Residual analysis {#model5residuals}

## Inference for regression

### Refresher: Professor evaluations data

Let's revisit the professor evaluations data that we analysized using multiple regression with one numerical and one categorical predictor. In particular

* $y$: outcome variable of instructor evaluation `score`
* predictor variables
    + $x_1$: numerical explanatory/predictor variable of `age`
    + $x_2$: categorical explanatory/predictor variable of `gender`
    
```{r}
library(ggplot2)
library(dplyr)

load(url("http://www.openintro.org/stat/data/evals.RData"))
evals <- evals %>%
  select(score, ethnicity, gender, language, age, bty_avg, rank)
```

First, recall that we had two competing potential models to explain professors'
teaching scores:

1. Model 1: No interaction term. i.e. both male and female profs have the same slope describing the associated effect of age on teaching score
1. Model 2: Includes an interaction term. i.e. we allow for male and female profs to have different slopes describing the associated effect of age on teaching score

### Refresher: Visualizations

Recall the plots we made for both these models:

```{r model1, echo=FALSE, warning=FALSE, fig.cap="Model 1: no interaction effect included"}
coeff <- lm(score ~ age + gender, data = evals) %>% coef() %>% as.numeric()
slopes <- evals %>%
  group_by(gender) %>%
  summarise(min = min(age), max = max(age)) %>%
  mutate(intercept = coeff[1]) %>%
  mutate(intercept = ifelse(gender == "male", intercept + coeff[3], intercept)) %>%
  gather(point, age, -c(gender, intercept)) %>%
  mutate(y_hat = intercept + age * coeff[2])
  
  ggplot(evals, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_line(data = slopes, aes(y = y_hat), size = 1)
```

```{r model2, echo=FALSE, warning=FALSE, fig.cap="Model 2: interaction effect included"}
ggplot(evals, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_smooth(method = "lm", se = FALSE)
```

### Refresher: Regression tables

Last, let's recall the regressions we fit. First, the regression with no 
interaction effect: note the use of `+` in the formula.

```{r, eval=FALSE}
get_regression_table(score ~ age + gender, data = evals)
```
```{r, echo=FALSE}
get_regression_table(score ~ age + gender, data = evals) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 1: Regression table with no interaction effect included", 
    booktabs = TRUE
  )
```

Second, the regression with an interaction effect: note the use of `*` in the formula.

```{r, eval=FALSE}
get_regression_table(score ~ age * gender, data = evals)
```
```{r, echo=FALSE}
get_regression_table(score ~ age * gender, data = evals) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 2: Regression table with interaction effect included", 
    booktabs = TRUE
  )
```


### New output: Confidence intervals

Let's recompute the regression tables, but with a new argument added: `tidy(conf.int=TRUE)`. This will add the 95% confidence intervals to our outputs. First, the regression with no 
interaction effect: note the use of `+` in the formula.

```{r, eval=FALSE}
get_regression_table(score ~ age + gender, data = evals)
```
```{r, echo=FALSE}
get_regression_table(score ~ age + gender, data = evals) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 1: Regression table with no interaction effect included", 
    booktabs = TRUE
  )
```

Second, the regression with an interaction effect: note the use of `*` in the formula.

```{r, eval=FALSE}
get_regression_table(score ~ age * gender, data = evals)
```
```{r, echo=FALSE}
get_regression_table(score ~ age * gender, data = evals) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 2: Regression table with interaction effect included", 
    booktabs = TRUE
  )
```



### Refresher: Residual analysis

Let's compute the residuals using `augment()` to see if there is a pattern.

```{r residual0, warning=FALSE}
regression_points <- get_regression_points(score ~ age * gender, data = evals)
regression_points
```

First the histogram:

```{r residual1, warning=FALSE, fig.cap="Model 2 (with interaction) histogram of residual"}
ggplot(regression_points, aes(x = residual)) +
  geom_histogram(binwidth = 0.25, color = "white") +
  labs(x = "Residual")
```

Second, the residuals as compared to the predictor variables:

* $x_1$: numerical explanatory/predictor variable of `age`
* $x_2$: categorical explanatory/predictor variable of `gender`

```{r residual2, warning=FALSE, fig.cap="Model 2 (with interaction) residuals vs predictor"}
ggplot(regression_points, aes(x = age, y = residual)) +
  geom_point() +
  labs(x = "age", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue", size = 1) +
  facet_wrap(~ gender)
```






## Other topics

### Correlation coefficient {#correlationcoefficient}

### Best fitting line {#leastsquares}

Regression lines are also known as "best fitting lines". But what do we mean by best? Let's unpack the criteria
that is used by regression to determine best. Recall the plot in Figure \@ref(fig:numxplot5) where for a professor
with a beauty average score of $x=7.333$

* The observed value $y=4.9$ was marked with a red circle
* The fitted value $\widehat{y} = 4.369$ on the regression line was marked with a red square
* The residual $y-\widehat{y} = 4.9-4.369 = 0.531$ was the length of the blue arrow.

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=2.333$. The residual in this case is $2.7 - 4.036 = -1.336$.

```{r echo=FALSE}
index <- which(evals$bty_avg == 2.333 & evals$score == 2.7)
target_point <- get_regression_points(score ~ bty_avg, data = evals) %>% 
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$score_hat
resid <- target_point$residual

best_fit_plot <- best_fit_plot +
  annotate("point", x = x, y = y, col = "red", size = 3) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 3) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))
best_fit_plot
```

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=3.667$. The residual in this case is $4.4 - 4.125 = 0.2753$.

```{r echo=FALSE}
index <- which(evals$bty_avg == 3.667 & evals$score == 4.4)
target_point <- get_regression_points(score ~ bty_avg, data = evals) %>% 
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$score_hat
resid <- target_point$residual

best_fit_plot <- best_fit_plot +
  annotate("point", x = x, y = y, col = "red", size = 3) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 3) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat,
           color = "blue", 
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))
best_fit_plot
```

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=6$. The residual in this case is $3.8 - 4.28 = -0.4802$.

```{r echo=FALSE}
index <- which(evals$bty_avg == 6 & evals$score == 3.8)
target_point <- get_regression_points(score ~ bty_avg, data = evals) %>% 
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$score_hat
resid <- target_point$residual

best_fit_plot <- best_fit_plot +
  annotate("point", x = x, y = y, col = "red", size = 3) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 3) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))
best_fit_plot
```


Now let's say we repeated this process for all `r nrow(evals)` professors in our
dataset. Regression *minimizes the sum of all `r nrow(evals)` arrow lengths
squared.* In other words, it minimizes the sum of the squared residuals:

$$
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
$$

We square the arrow lengths so that positive and negative deviations of the same amount are treated equally.  That's why alternative names for the simple linear regression line are the **least-squares line** and the **best fitting line**. It can be proven via calculus and linear algebra that this line uniquely minimizes the sum of the squared arrow lengths. 

For the regression line in the plot, the sum of the squared residuals is `r get_regression_points(score~bty_avg, data=evals) %>% summarise(SSE = sum(residual^2)) %>% pull(SSE) %>% round(3)`.


### How does `get_regression_table()` work? {#underthehood}