# Data Modeling with Regression {#regression}

```{r setup_reg, include=FALSE, purl=FALSE}
chap <- 6
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth'
  )
options(scipen = 99, digits = 4)

# This bit of code is a bug fix on asis blocks, which we use to show/not show LC
# solutions, which are written like markdown text. In theory, it shouldn't be
# necessary for knitr versions <=1.11.6, but I've found I still need to for
# everything to knit properly in asis blocks. More info here: 
# https://stackoverflow.com/questions/32944715/conditionally-display-block-of-markdown-text-using-knitr
library(knitr)
knit_engines$set(asis = function(options) {
  if (options$echo && options$eval) knit_child(text = options$code)
})

# This controls which LC solutions to show. Options for solutions_shown: "ALL"
# (to show all solutions), or subsets of c('5-1', '5-2','5-3', '5-4'), including
# the null vector c('') to show no solutions.
solutions_shown <- c('')
show_solutions <- function(section){
  return(solutions_shown == "ALL" | section %in% solutions_shown)
  }
```

Now that we are equipped with data visualization skills from Chapter \@ref(viz), data wrangling skills from Chapter \@ref(wrangling), and an understanding of the "tidy" data format from Chapter \@ref(tidy), we now proceed to discuss once of the most commonly used statistical procedures: *regression*.  Much as we saw with the Grammar of Graphics in Chapter \@ref(viz), the fundamental premise of (simple linear) regression is to *model* the relationship between 

* An outcome/dependent/predicted variable $y$
* As a function of a covariate/independent/predictor variable $x$

Why do we have multiple labels for the same concept? Whatâ€™s their root? Regression, in its simplest form, can be viewed in two ways:

1. **For Prediction**: You want to predict an outcome variable $y$ based on the information contained in a set of predictor variables. You don't care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about $y$, you're fine. 
1. **For Explanation**: You want to study the relationship between an outcome variable $y$ and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these.

In this chapter, we use the `flights` data frame in the `nycflights13` package to look at the relationship between departure delay, arrival delay, and other variables related to flights.  We will also discuss the concept of *correlation* and how it is frequently incorrectly implied to also lead to *causation*. This chapter also introduces the `broom` package, which is a useful tool for summarizing the results of regression fits in "tidy" format.

### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r message=FALSE, warning=FALSE}
library(nycflights13)
library(ggplot2)
library(dplyr)
library(broom)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(mvtnorm) 
```






## Datasets

### Professor evaluations

A study conducted at the University of Texas in Austin
available at [openintro.org](https://www.openintro.org/stat/data/?data=evals) 
investigated associations between teacher evalutions and
various attributes of the instructor. 

* **Question**: Can we explain a professor's teaching evaluation score using these
attritubes?
* Outcome variable $y$: Average teaching score, based on students evaluations between 1 and 5
* Explanatory variables $x$
    + their rank: teaching, tenure track, or tenured
    + their ethnicity: minority or non-minority
    + their (binary) gender: male or female
    + their language: whether or not english was their mother tongue
    + their age:
    + their average "beauty" rating, based on a panel of 6 students' scores between 1 and 10.

First let's load the data and look at a random sample of 5 rows:

```{r}
load(url("http://www.openintro.org/stat/data/evals.RData"))
evals <- evals %>%
  select(score, ethnicity, gender, language, age, bty_avg, rank)
```

```{r, echo=FALSE}
evals %>% 
  sample_n(5) %>% 
  kable(
    digits=2,
    caption = "Random sample of 5 instructors", 
    booktabs = TRUE
  )
```


## One numerical x: Professor evaluations

### Exploratory data analysis

Let's

* Model the outcome variable y: teacher evaluation score
* Using explanatory variable x: beauty score

Let's plot the data:

```{r numxplot1, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score")
```

The above suffers from *overplotting*; let's break it up with a little random jitter added to the points. Note, we are
only altering the visualization of the points; the original data is the same.

```{r numxplot2, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: Jittered"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_jitter() + 
  labs(x="Beauty Score", y="Teaching Score")
```


### Correlation coefficient

The correlation coefficient between teacher evaluation score and beauty score is
high:

```{r, eval=FALSE}
cor(evals$score, evals$bty_avg)
```
```{r, echo=FALSE}
evals %>% 
  summarize(correlation = cor(score, bty_avg)) %>% 
  kable(
    digits=3,
    caption = "Correlation coefficient between teaching score and beauty score", 
    booktabs = TRUE
  )
```


### Simple linear regression

Let's

* Fit a simple linear regression using the command `lm(y ~ x, data)` below:
* Output the table in tidy data format using the `tidy()` function from the `broom` package.

```{r, eval=FALSE}
lm(score~bty_avg, data=evals) %>% 
  tidy()
```
```{r, echo=FALSE}
lm(score~bty_avg, data=evals) %>% 
  tidy() %>% 
  kable(
    digits=3,
    caption = "Regression line estimates", 
    booktabs = TRUE
  )
```

Let's plot the regression line with a new layer called `+ geom_smooth(method="lm")`:

```{r numxplot3, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: With regression line"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm")
```

Let's ignore the grey bars now (which are "error" bars) by adding the argument `se=FALSE` to `geom_smooth`

```{r numxplot4, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: With regression line, no error bars"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE)
```



### Observed values, fitted values, and residuals

```{r numxplot5, echo=FALSE, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: With regression line, no error bars"}
x <- 7.333
y <- 4.9
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)

ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE) +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
```

* Red point: *Observed point* $(x,y) = (\mbox{beauty score}, \mbox{teaching score}) = (7.333, 4.9)$. *Observed value* $y=4.9$
* Red square: *Fitted point* $(x,\widehat{y}) = (\mbox{beauty score}, \mbox{fitted teaching score}) = (7.333, 4.371)$. *Fitted value* $\widehat{y} = 4.371$. Why 4.371? From table above: $\widehat{y} = b_0 + b_1 x = 3.880 + 0.067 \times 7.333$
* Length of blue arrow: *Residual* (i.e. error) $y - \widehat{y} = 4.9 - 4.371 = 0.529$



