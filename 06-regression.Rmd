# Data Modeling with Regression {#regression}

```{r setup_reg, include=FALSE, purl=FALSE}
chap <- 6
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth'
  )
options(scipen = 99, digits = 4)

# This bit of code is a bug fix on asis blocks, which we use to show/not show LC
# solutions, which are written like markdown text. In theory, it shouldn't be
# necessary for knitr versions <=1.11.6, but I've found I still need to for
# everything to knit properly in asis blocks. More info here: 
# https://stackoverflow.com/questions/32944715/conditionally-display-block-of-markdown-text-using-knitr
library(knitr)
knit_engines$set(asis = function(options) {
  if (options$echo && options$eval) knit_child(text = options$code)
})

# This controls which LC solutions to show. Options for solutions_shown: "ALL"
# (to show all solutions), or subsets of c('5-1', '5-2','5-3', '5-4'), including
# the null vector c('') to show no solutions.
solutions_shown <- c('')
show_solutions <- function(section){
  return(solutions_shown == "ALL" | section %in% solutions_shown)
  }
```

Now that we are equipped with data visualization skills from Chapter \@ref(viz), data wrangling skills from Chapter \@ref(wrangling), and an understanding of the "tidy" data format from Chapter \@ref(tidy), we now proceed to discuss once of the most commonly used statistical procedures: *regression*.  Much as we saw with the Grammar of Graphics in Chapter \@ref(viz), the fundamental premise of (simple linear) regression is to *model* the relationship between 

* An outcome/dependent/predicted variable $y$
* As a function of a covariate/independent/predictor variable $x$

Why do we have multiple labels for the same concept? Whatâ€™s their root? Regression, in its simplest form, can be viewed in two ways:

1. **For Prediction**: You want to predict an outcome variable $y$ based on the information contained in a set of predictor variables. You don't care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about $y$, you're fine. 
1. **For Explanation**: You want to study the relationship between an outcome variable $y$ and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these.

In this chapter, we use the `flights` data frame in the `nycflights13` package to look at the relationship between departure delay, arrival delay, and other variables related to flights.  We will also discuss the concept of *correlation* and how it is frequently incorrectly implied to also lead to *causation*. This chapter also introduces the `broom` package, which is a useful tool for summarizing the results of regression fits in "tidy" format.

### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r message=FALSE, warning=FALSE}
library(nycflights13)
library(ggplot2)
library(dplyr)
library(broom)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(mvtnorm) 
library(tidyr)
```






## Datasets

### Professor evaluations

A study conducted at the University of Texas in Austin
available at [openintro.org](https://www.openintro.org/stat/data/?data=evals) 
investigated associations between teacher evalutions and
various attributes of the instructor. 

* **Question**: Can we explain a professor's teaching evaluation score using these
attritubes?
* Outcome variable $y$: Average teaching score, based on students evaluations between 1 and 5
* Explanatory variables $x$
    + their rank: teaching, tenure track, or tenured
    + their ethnicity: minority or non-minority
    + their (binary) gender: male or female
    + their language: whether or not english was their mother tongue
    + their age:
    + their average "beauty" rating, based on a panel of 6 students' scores between 1 and 10.

First let's load the data and look at a random sample of 5 rows:

```{r}
load(url("http://www.openintro.org/stat/data/evals.RData"))
evals <- evals %>%
  select(score, ethnicity, gender, language, age, bty_avg, rank)
```

```{r, echo=FALSE}
evals %>% 
  sample_n(5) %>% 
  kable(
    digits=2,
    caption = "Random sample of 5 instructors", 
    booktabs = TRUE
  )
```


### Country-by-country life expectancies in 2007 {#datalifeexp}

The `gapminder` package contains the `gapminder` data frame, which contains
development data about various countries for 5-year intervals between 1952 and 2007

* **Question**: What variables are associated with life expectancy for all
countries in 2007? In other words, can we identify variables that can explain
variation in life expectancy between countries.
* Outcome variable $y$: Mean life expectancy
* Explanatory variables $x$
    + continent
    + GDP per capita

First let's load the data and look at a random sample of 5 rows:

```{r}
library(gapminder)
gapminder2007 <- gapminder %>%
  filter(year == 2007) %>% 
  select(country, lifeExp, continent, gdpPercap)
```

```{r, echo=FALSE}
gapminder2007 %>% 
  sample_n(5) %>% 
  kable(digits=1)
```






## One numerical x: Professor evaluations

### Exploratory data analysis

Let's

* Model the outcome variable y: teacher evaluation score
* Using explanatory variable x: beauty score

Let's plot the data:

```{r numxplot1, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score")
```

The above suffers from *overplotting*; let's break it up with a little random jitter added to the points. Note, we are
only altering the visualization of the points; the original data is the same.

```{r numxplot2, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: Jittered"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_jitter() + 
  labs(x="Beauty Score", y="Teaching Score")
```


### Correlation coefficient

The correlation coefficient between teacher evaluation score and beauty score is
high:

```{r, eval=FALSE}
cor(evals$score, evals$bty_avg)
```
```{r, echo=FALSE}
evals %>% 
  summarize(correlation = cor(score, bty_avg)) %>% 
  kable(
    digits=3,
    caption = "Correlation coefficient between teaching score and beauty score", 
    booktabs = TRUE
  )
```


### Simple linear regression

Let's

* Fit a simple linear regression using the command `lm(y ~ x, data)` below:
* Output the table in tidy data format using the `tidy()` function from the `broom` package.

```{r, eval=FALSE}
lm(score~bty_avg, data=evals) %>% 
  tidy()
```
```{r, echo=FALSE}
lm(score~bty_avg, data=evals) %>% 
  tidy() %>% 
  kable(
    digits=3,
    caption = "Regression line estimates", 
    booktabs = TRUE
  )
```

Let's plot the regression line with a new layer called `+ geom_smooth(method="lm")`:

```{r numxplot3, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: With regression line"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm")
```

Let's ignore the grey bars now (which are "error" bars) by adding the argument `se=FALSE` to `geom_smooth`

```{r numxplot4, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: With regression line, no error bars"}
ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE)
```



### Observed values, fitted values, and residuals

```{r numxplot5, echo=FALSE, warning=FALSE, fig.cap="Teacher evaluation scores at UT Austin: Example of fitted value and residual"}
x <- 7.333
y <- 4.9
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat

ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE) +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
```

* Red point: *Observed point* $(x,y) = (\mbox{beauty score}, \mbox{teaching score}) = (7.333, 4.9)$. *Observed value* $y=4.9$
* Red square: *Fitted point* $(x,\widehat{y}) = (\mbox{beauty score}, \mbox{fitted teaching score}) = (7.333, 4.369)$. *Fitted value* $\widehat{y} = 4.369$. Why 4.371? From table above: $\widehat{y} = b_0 + b_1 x = 3.880 + 0.067 \times 7.333 = 4.369$
* Length of blue arrow: *Residual* (i.e. error) $y - \widehat{y} = 4.9 - 4.369 = 0.531$

Let's extract point-by-point information now:

* for all `r nrow(evals)` points in our plot, in other words
* for all `r nrow(evals)` instructors in the study, in other words
* for all `r nrow(evals)` rows in the `evals` data frame

using the `augment()` function from the `broom` package. This function
*augments* the original data by adding new information resulting from the
regression. Let's look at 5 arbitrarily chosen rows. (Note that the first row in
the table corresponds to the red points and blue arrow in Figure
\@ref(fig:numxplot5)):

```{r, eval=FALSE}
point_by_point_info <- lm(score~bty_avg, data=evals) %>% 
  augment() %>% 
  select(score, bty_avg, .fitted, .resid)
point_by_point_info
```
```{r, echo=FALSE}
set.seed(76)
point_by_point_info <- lm(score~bty_avg, data=evals) %>% 
  augment() %>% 
  select(score, bty_avg, .fitted, .resid)
point_by_point_info %>% 
  slice(c(21, sample(1:nrow(evals), 4))) %>% 
  kable(
    digits=3,
    caption = "Point-by-point information (5 of 463 rows)", 
    booktabs = TRUE
  )
```

The periods in the variable names indicated that they are new information that
was added to the original dataset. In particular

* **.fitted**: the fitted value $\widehat{y}$ corresponding to an observed point $(x,y)$. One 
example is the red square in Figure \@ref(fig:numxplot5)
* **.resid**: the residual $y - \widehat{y}$. One example is the length of the blue arrow
in Figure \@ref(fig:numxplot5).




### Residual analysis

Recall the residuals can be thought of as "lack-of-fit", or "left-overs", or
"errors". Ideally, we want there to be *no systematic pattern* to the residuals. 
What does this mean?

1. The residuals should be on average 0. 
1. The spread of the residuals should be consistent.
1. The value of the residuals should not depend on the value of x

Let's look at the residuals two ways. First, recall in Figure \@ref(fig:numxplot5)
above we plotted:

* On the y-axis: $y$ teaching score
* On the x-axis: $x$ beauty score

Instead, in Figure \@ref(fig:numxplot6) below let's plot

* On the y-axis: $y-\widehat{y}$ residual instead
* On the x-axis: $x$ beauty score (same as before)

```{r numxplot6, echo=FALSE, warning=FALSE, fig.cap="Plot of residuals over beauty score"}
ggplot(point_by_point_info, aes(x=bty_avg, y=.resid)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Residual") +
  geom_hline(yintercept=0, col="blue", size =1) +
  annotate("point", x=x, y=resid, col="red", size=3) +
  annotate("point", x=x, y=0, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=resid, yend=0, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
```

You can think of Figure \@ref(fig:numxplot6) as Figure \@ref(fig:numxplot5), but
with the blue line flattened out. Does it seem like there is *no systematic pattern*
to the residuals?

Here are some hypothetical examples where there is a systematic pattern to the
residuals:

```{r numxplot7, echo=FALSE, warning=FALSE, fig.cap="Examples of less than ideal residual patterns"}
temp <- evals
temp$`Example 1` <- ((evals$bty_avg-5)^2 - 6 + rnorm(nrow(evals), 0, 0.5))*0.4
temp$`Example 2` <- (rnorm(nrow(evals), 0, 0.075*evals$bty_avg^2))*0.4

temp <- temp %>%
  select(bty_avg, `Example 1`, `Example 2`) %>% 
  gather(type, eps, -bty_avg)

ggplot(temp, aes(x=bty_avg, y=eps)) +
  geom_point() + 
  labs(x="Beauty Score", y="Residual") +
  geom_hline(yintercept=0, col="blue", size =1) +
  facet_wrap(~type)
```

The second way to look at the residuals is using a histogram:

```{r numxplot8, warning=FALSE, fig.cap="Histogram of residuals"}
ggplot(point_by_point_info, aes(x=.resid)) +
  geom_histogram(binwidth = 0.25) +
  labs(x="Residual")
```

This histogram seems to indicate that we have more positive residuals than negative.
Since Residual  = $y-\widehat{y} > 0$ when $y > \widehat{y}$, it seems
our fitted professor score from the regression model have a slight tendency of
being small than the actually observed score.

This histogram has a slight *left-skew* in that there is a long tail on the
left. Another way to say this is this data exhibit a *negative skew*. Here are examples
of an ideal and less than ideal patterns of residuals.

```{r numxplot9, echo=FALSE, warning=FALSE, fig.cap="Examples of ideal and less than ideal residual patterns"}
temp <- evals
temp$`Ideal` <- rnorm(nrow(temp), 0, sd=sd(point_by_point_info$.resid))
temp$`Less than ideal` <- rnorm(nrow(temp), 0, sd=sd(point_by_point_info$.resid))^2

temp <- temp %>%
  select(bty_avg, `Ideal`, `Less than ideal`) %>% 
  gather(type, eps, -bty_avg)

ggplot(temp, aes(x=eps)) +
  geom_histogram(binwidth = 0.25) +
  labs(x="Residual") +
  facet_wrap(~type, scales = "free")
```

In fact, we'll see later on that we would like the residuals to be *normally distributed* with
mean 0. In other words, be bell-shaped and centered at 0!



### Best fitting lines

Regression lines are also known as "best fitting lines". But what do we mean by best? Let's unpack the criteria
that is used by regression to determine best. Recall the plot in Figure \@ref(fig:numxplot5)) where for a professor
with a beauty average score of $x=7.333$

* The observed value $y=4.9$ was marked with a red circle
* The fitted value $\widehat{y} = 4.369$ on the regression line was marked with a red square
* The residual $y-\widehat{y} = 4.9-4.369 = 0.531$ was the length of the blue arrow.

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=2.333$. The residual in this case is $2.7 - 4.036 = -1.336$.

```{r echo=FALSE}
x <- 7.333
y <- 4.9
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat

best_fit_plot <- ggplot(evals, aes(x=bty_avg, y=score)) + 
  geom_point() + 
  labs(x="Beauty Score", y="Teaching Score") +
  geom_smooth(method="lm", se=FALSE) +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 

x <- 2.333
y <- 2.7
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat
best_fit_plot <- best_fit_plot +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
best_fit_plot
```

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=3.667$. The residual in this case is $4.4 - 4.125 = 0.2753$.

```{r echo=FALSE}
x <- 3.667
y <- 4.4
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat
best_fit_plot <- best_fit_plot +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
best_fit_plot
```

Let's do this for another arbitrarily chosen professor whose beauty score was
$x=6$. The residual in this case is $3.8 - 4.28 = -0.4802$.

```{r echo=FALSE}
x <- 6
y <- 3.8
y_hat <- coef(lm(score~bty_avg, data=evals)) %>% as.numeric()
y_hat <- y_hat * c(1, x) 
y_hat <- sum(y_hat)
resid <- y - y_hat
best_fit_plot <- best_fit_plot +
  annotate("point", x=x, y=y, col="red", size=3) +
  annotate("point", x=x, y=y_hat, col="red", shape=15, size=3) +
  annotate("segment", x=x, xend=x, y=y, yend=y_hat, color = "blue", arrow = arrow(type="closed", length = unit(0.02, "npc"))) 
best_fit_plot
```


Now let's say we repeated this process for all `r nrow(evals)` professors in our
dataset. Regression *minimizes the sum of all `r nrow(evals)` arrow lengths
squared.* In other words, it minimizes the sum of the squared residuals:

$$
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
$$

We square the arrow lengths so that positive and negative deviations of the same amount are treated equally.  That's why alternative names for the simple linear regression line are the **least-squares line** and the **best fitting line**. It can be proven via calculus and linear algebra that this line uniquely minimizes the sum of the squared arrow lengths. 

For the regression line in the plot, the sum of the squared residuals is `r lm(score~bty_avg, data=evals) %>% augment() %>% summarise(SSE = sum(.resid^2)) %>% pull(SSE) %>% round(3)`.



 





## One categorical x: Professor evaluations

### Exploratory data analysis

Let's load the country-by-country life expectancy data available in 
Section \@ref(datalifeexp).

```{r, eval=TRUE}
lifeExp_by_continent <- gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(median = median(lifeExp))
```
```{r, echo=FALSE}
lifeExp_by_continent %>% 
  kable(
    digits=2,
    caption = "Life expectancy by continent", 
    booktabs = TRUE
  )
lifeExp_by_continent <- gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(median = median(lifeExp))

# What was the median life expectancy in Africa in 2007?
median_africa <- lifeExp_by_continent %>% 
  filter(continent == "Africa") %>%
  pull(median)
```

The median life expectancy in Africa in was 52.93. Let's look at a boxplot
of life expectancy and mark the median for Africa in red.

```{r catxplot1, warning=FALSE, fig.cap="Life expectancy in 2007"}
ggplot(gapminder2007, aes(x=continent, y=lifeExp)) +
  geom_boxplot() +
  geom_hline(yintercept = 52.93, col="red", size=1) +
  labs(x = "Continent", y="Life expectancy (years)")
```

Let's look nubmer of years difference

```{r catxplot2, warning=FALSE, fig.cap="Difference in life expectancy relative to African median of 52.93 years"}
ggplot(gapminder2007, aes(x=continent, y=lifeExp-52.93)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, col="red", size=1) +
  labs(x="Continent", y="Difference in life expectancy (years)")
```


### Regression 

```{r, eval=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp))
```
```{r, echo=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp)) %>% 
  kable(
    digits=2,
    caption = "Continent means", 
    booktabs = TRUE
  )
```

```{r, eval=FALSE}
lm(lifeExp ~ continent, data=gapminder2007) %>% 
  tidy()
```
```{r, echo=FALSE}
lm(lifeExp ~ continent, data=gapminder2007) %>% 
  tidy() %>% 
  kable(
    digits=2,
    caption = "Regression table", 
    booktabs = TRUE
  )
```


```{r, eval=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp)) %>% 
  mutate(compare_to_africa = round(mean - 54.81,2))
```
```{r, echo=FALSE}
gapminder2007 %>% 
  group_by(continent) %>% 
  summarize(mean = mean(lifeExp)) %>% 
  mutate(compare_to_africa = mean - 54.81) %>% 
  kable(
    digits=2,
    caption = "Relative to baseline", 
    booktabs = TRUE
  )
```
