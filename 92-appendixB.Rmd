# Inference Examples {#appendixB}

## Randomization

We will now focus on building hypotheses looking at the difference between two population means in an example.  We will denote population means using the Greek symbol $\mu$ (pronounced "mu").  Thus, we will be looking to see if one group "out-performs" another group.  This is quite possibly the most common type of statistical inference and serves as a basis for many other types of analyses when comparing two groups. 

Our null hypothesis will be of the form $H_0: \mu_1 = \mu_2$, which can also be written as $H_0: \mu_1 - \mu_2 = 0$.  Our alternative hypothesis will be of the form $H_0: \mu_1 \star \mu_2$ (or $H_a:  \mu_1 - \mu_2 \, \star \, 0$) where $\star$ = $<$, $\ne$, or $>$  depending on the context of the problem.  You needn't focus on these new symbols too much at this point.  It will just be a shortcut way for us to describe our hypotheses.

As we saw in Chapter \@ref(infer-basics), simulation and bootstrapping are valuable tools when conducting inferences based on one population variable.  We will see that the process of **randomization**, which is a resampling procedure similar to bootstrapping in some ways, will be valuable in conducting tests comparing quantitative values from two groups.


### Comparing Action and Romance Movies

The `movies` data set in the `ggplot2movies` package contains information on a large number of movies that have been rated by users of IMDB.com.  We are interested in the question here of whether `Action` movies are rated higher on IMDB than `Romance` movies.  We will first need to do a little bit of data manipulation using the ideas from Chapter \@ref(manip) to get the data in the form that we would like:

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2movies)
(movies_trimmed <- movies %>% select(title, year, rating, Action, Romance))
```

Note that `Action` and `Romance` are binary variables here.  To remove any overlap of movies (and potential confusion) that are both `Action` and `Romance`, we will remove them from our _population_:

```{r}
movies_trimmed <- movies_trimmed %>%
  filter(!(Action == 1 & Romance == 1))
```

We will now create a new variable called `genre` that specifies whether a movie in our `movies_trimmed` data frame is an `"Action"` movie, a `"Romance"` movie, or `"Neither"`.  We aren't really interested in the `"Neither"` category here so we will exclude those rows as well.  Lastly, the `Action` and `Romance` columns are not needed anymore since they are encoded in the `genre` column.

```{r}
movies_trimmed <- movies_trimmed %>%
  mutate(genre = ifelse(Action == 1, "Action",
                        ifelse(Romance == 1, "Romance",
                               "Neither"))) %>%
  filter(genre != "Neither") %>%
  select(-Action, -Romance)
```
  
We are left with `r nrow(movies_trimmed)` movies in our _population_ data set that focuses on only `"Action"` and `"Romance"` movies.  
    
***
```{block lc7-2, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are the different genre variables stored as binary variables (1s and 0s) instead of just listing the `genre` as a column of values like "Action", "Comedy", etc.?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What complications could come above with us excluding action romance movies?  Should we question the results of our hypothesis test?  Explain.

***    

Let's now visualize the distributions of `rating` across both levels of `genre`.  Think about what type(s) of plot is/are appropriate here before you proceed:

```{r fig.cap="Rating vs genre in the population"}
library(ggplot2)
movies_trimmed %>% ggplot(aes(x = genre, y = rating)) +
  geom_boxplot()
```

We can see that the middle 50% of ratings for `"Action"` movies is more spread out than that of `"Romance"` movies in the population.  `"Romance"` has outliers at both the top and bottoms of the scale though.  We are initially interested in comparing the mean `rating` across these two groups so a faceted histogram may also be useful:

```{r warning=FALSE, fig.cap="Faceted histogram of genre vs rating"}
movies_trimmed %>% ggplot(mapping = aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white", fill = "dodgerblue") +
  facet_grid(genre ~ .)
```

**Important note:** Remember that we hardly ever have access to the population values as we do here.  This example and the `nycflights13` data set were used to create a common flow from chapter to chapter.  In nearly all circumstances, we'll be needing to use only a sample of the population to try to infer conclusions about the unknown population parameter values.  These examples do show a nice relationship between statistics (where data is usually small and more focused on experimental settings) and data science (where data is frequently large and collected without experimental conditions). 

<!--
We'll learn more about observational studies and experiments in Chapter \@ref(regress).
-->

### Sampling $\rightarrow$ Randomization
    
We can use hypothesis testing to investigate ways to determine, for example, whether a **treatment** has an effect over a **control** and other ways to statistically analyze if one group performs better than, worse than, or different than another.  We will also use confidence intervals to determine the size of the effect if it exists. You'll see more on this in Chapter \@ref(ci).

We are interested here in seeing how we can use a random sample of action movies and a random sample of romance movies from `movies` to determine if a statistical difference exists in the mean ratings of each group.

***
```{block lc7-3a, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Define the relevant parameters here in terms of the populations of movies.

***  

Let's select a random sample of 34 action movies and a random sample of 34 romance movies.  (The number 34 was chosen somewhat arbitrarily here.)

```{r}
library(dplyr)
library(mosaic)
set.seed(2016)
movies_genre_sample <- movies_trimmed %>% 
  group_by(genre) %>%
  sample_n(34)
```

We can now observe the distributions of our two sample ratings for both groups.  Remember that these plots should
be rough approximations of our population distributions of movie ratings for `"Action"` and `"Romance"` in our population of all movies in the `movies` data frame.

```{r fig.cap="Genre vs rating for our sample"}
movies_genre_sample %>% ggplot(aes(x = genre, y = rating)) +
  geom_boxplot()
```

```{r warning=FALSE, fig.cap="Genre vs rating for our sample as faceted histogram"}
movies_genre_sample %>% ggplot(mapping = aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white", fill = "dodgerblue") +
  facet_grid(genre ~ .)
```

***
```{block lc7-3b1, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What single value could we change to improve the approximation using the sample distribution on the population distribution?

***

Do we have reason to believe, based on the sample distributions of `rating` over the two groups of `genre`, that there is a significant difference between the mean `rating` for action movies compared to romance movies?  It's hard to say just based on the plots.  The boxplot does show that the median sample rating is higher for romance movies, but the histogram isn't as clear.  The two groups have somewhat differently shaped distributions but they are both over similar values of `rating`.  It's often useful to calculate the mean and standard deviation as well, conditioned on the two levels.

```{r}
summary_ratings <- movies_genre_sample %>% 
  group_by(genre) %>%
  summarize(mean = mean(rating),
            std_dev = sd(rating))
summary_ratings
```

***
```{block lc7-3b2, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why did we not specify `na.rm = TRUE` here as we did in Chapter \@ref(manip)?

***

We see that the sample mean rating for romance movies, $\bar{x}_{r}$, is greater than the similar measure for action movies, $\bar{x}_a$.  But is it statistically significantly greater (thus, leading us to conclude that the means are statistically different)?  The standard deviation can provide some insight here but with these standard deviations being so similar it's still hard to say for sure.

***
```{block lc7-3b3, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why might the standard deviation provide some insight about the means being statistically different or not?

***

The hypotheses we specified can also be written in another form to better give us an idea of what we will be simulating to create our null distribution.

- $H_0: \mu_r - \mu_a = 0$
- $H_a: \mu_r - \mu_a \ne 0$
  
We are, therefore, interested in seeing whether the difference in the sample means, $\bar{x}_r - \bar{x}_a$, is statistically different than 0. R has a built-in command that can calculate the difference in these two sample means.

```{r}
mean_ratings <- movies_genre_sample %>% group_by(genre) %>%
  summarize(mean = mean(rating))
obs_diff <- diff(mean_ratings$mean)
```

We see here that the `diff` function calculates $\bar{x}_r - \bar{x}_a = `r mean_ratings$mean[2]` - `r mean_ratings$mean[1]` = `r obs_diff`$.  We will now proceed similarly to how we conducted hypothesis tests in Chapter \@ref(infer-basics) using simulation.  Our goal is figure out a random process with which to simulate the null hypothesis being true.  In Chapter \@ref(infer-basics), we used flipping of a fair coin as the random process we were simulating with the null hypothesis being true ($H_0: \pi = 0.5$).

Here, with us assuming the two population means are equal ($H_0: \mu_r - \mu_a = 0$), we can look at this from a tactile point of view by using index cards.  There are $n_r = 34$ data elements corresponding to romance movies and $n_a = 34$ for action movies.  We can write the 34 ratings from our sample for romance movies on one set of 34 index cards and the 34 ratings for action movies on another set of 34 index cards.  (Note that the sample sizes need not be the same.)

The next step is to put the two stacks of index cards together, creating a new set of 68 cards.  If we assume that the two population means are equal, we are saying that there is no association between ratings and genre (romance vs action).  We can use the index cards to create two **new** stacks for romance and action movies.  First, we must shuffle all the cards thoroughly.  After doing so, in this case with equal values of sample sizes, we split the deck in half.  (You should be thinking about how this process is similar to what was done with **bootstrapping** in Chapter \@ref(infer-basics).)

We then calculate the new sample mean rating of the romance deck, and also the new sample mean rating of the action deck.  This creates one simulation of the samples that were collected originally.  We next want to calculate a statistic from these two samples.  Instead of actually doing the calculation using index cards, we can use R as we have before to simulate this process.

***
```{block lc7-3b4, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** How would the tactile shuffling of index cards change if we had different samples of say 20 action movies and 60 romance movies?  Describe each step that would change.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are we taking the difference in the means of the cards in the new shuffled decks?

***

```{r message=FALSE, warning=FALSE}
library(mosaic)
shuffled_ratings <- movies_trimmed %>%
     mutate(rating = shuffle(rating)) %>% 
     group_by(genre) %>%
     summarize(mean = mean(rating))
diff(shuffled_ratings$mean)
```

The only new command here is `shuffle` from the `mosaic` package, which does what we would expect it to do.  It simulates a shuffling of the ratings between the two levels of `genre` just as we could have done with index cards.  We can now proceed in a similar way to what we have done previously in Chapter \@ref(infer-basics) by repeating this process many times to create a _null distribution_ of simulated differences in sample means.

```{r cache=TRUE}
set.seed(2016)
many_shuffles <- do(10000) * 
  (movies_trimmed %>%
     mutate(rating = shuffle(rating)) %>% 
     group_by(genre) %>%
     summarize(mean = mean(rating))
   )
```

It is a good idea here to `View` the `many_shuffles` data frame via `View(many_shuffles)`.  We need to figure out a way to subtract the first value of `mean` from the second value of `mean` for each of the 10,000 simulations.  This is a little tricky but the `group_by` function comes to our rescue here:

```{r}
rand_distn <- many_shuffles %>%
  group_by(.index) %>%
  summarize(diffmean = diff(mean))
```

We can now plot the distribution of these simulated differences in means:

```{r fig.cap="Simulated differences in means histogram"}
rand_distn %>% ggplot(aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 20)
```

Remember that we are interested in seeing where our observed sample mean difference of `r diff(mean_ratings$mean)` falls on this null/randomization distribution.  We are interested in simply a difference here so "more extreme" corresponds to values in both tails on the distribution.  Let's shade our null distribution to show a visual representation of our $p$-value:

```{r fig.cap="Shaded histogram to show p-value"}
rand_distn %>% ggplot(aes(x = diffmean, fill = (abs(diffmean) >= obs_diff))) +
  geom_histogram(color = "white", bins = 20)
```

You may initially think there is an error here, but remember that the observed difference in means was `r obs_diff`.  It falls far outside the range of simulated differences.  We can add a vertical line to represent both it and its negative (since this is a two-tailed test) instead:

```{r fig.cap="Histogram with vertical lines corresponding to observed statistic"}
rand_distn %>% ggplot(aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 100) +
  geom_vline(xintercept = obs_diff, color = "red") +
  geom_vline(xintercept = -obs_diff, color = "red")
```

Based on this plot, we have evidence supporting the conclusion that the mean rating for romance movies is different from that of action movies.  (It doesn't really matter what significance level was chosen in this case.  Think about why.)  The next important idea is to better understand just how much higher of a mean rating can we expect the romance movies to have compared to that of action movies.  This can be addressed by creating a 95% confidence interval as we will explore in Chapter \@ref(ci).
  
***

```{block lc7-3b, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Conduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating?  Make sure to use the `%>%` as much as possible.  What was different and what was the same? 

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What conclusions can you make from viewing the faceted histogram looking at `rating` versus `genre` that you couldn't see when looking at the boxplot?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Describe in a paragraph how we used Allen Downey's diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Using the definition of "$p$-value" in Chapter \@ref(infer-basics), write in words what the $p$-value represents for the hypothesis test above comparing the mean rating of romance to action movies.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What is the value of the $p$-value for the hypothesis test comparing the mean rating of romance to action movies?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Do the results of the hypothesis test match up with the original plots we made looking at the population of movies?  Why or why not?

***  

### Summary

To review, these are the steps one would take whenever you'd like to do a hypothesis test comparing
values from the distributions of two groups:

- Simulate many samples using a random process that matches the way
the original data were collected and that _assumes the null hypothesis is
true_. 

- Collect the values of a sample statistic for each sample created using this random process to build
a _randomization distribution_.

- Assess the significance of the _original_ sample by determining where
its sample statistic lies in the randomization distribution.

- If the proportion of values as extreme or more extreme than the observed statistic in the randomization
distribution is smaller than the pre-determined significance level $\alpha$, we reject $H_0$.  Otherwise,
we fail to reject $H_0$.  (If no significance level is given, one can assume $\alpha = 0.05$.)

<!--
## Theory-based methods {#theory-hypo} (Put into lab instead)

## Non-rejection of null hypothesis (Put into lab instead)
-->

## Bootstrapping

Just as we did in the previous section when making hypotheses about a population proportion with which we would like to test which one is more plausible, we can also use simulation to infer conclusions about a population quantitative statistic such as the mean.  In this case, we will focus on constructing confidence intervals to produce plausible values for a population mean.  (We can do a similar analysis for a population median or other summary measure as well.)

Traditionally, the way to construct confidence intervals for a mean is to assume a normal distribution for the population or to invoke the Central Limit Theorem and get, what often appears to be magic, results.  These methods are often not intuitive, especially for those that lack a strong mathematical background.  They also come with their fair share of assumptions and often turn Statistics, a field that is full of tons of useful applications to many different fields and disciplines, into a robotic procedural-based topic.  It doesn't have to be that way!

In this section, we will introduce the concept of **bootstrapping**.  It will be a useful tool that will allow us to estimate the variability of our statistic from sample to sample.  One neat feature of bootstrapping is that it enables us to approximate the sampling distribution and estimate the distribution's standard deviation using ONLY the information in the one selected (original) sample.  

It sounds just as plagued with the magical type qualities of traditional theory-based inference on initial glance but we will see that it provides an intuitive and useful way to make inferences, especially when the samples are of medium to large size.  

<!--We will begin by investigating an example on the selling prices of used Ford Mustang cars taken from the textbook {\it Statistics:  UnLOCKing the Power of Data} by Lock, Lock, Lock, Lock, and Lock.  (That isn't me hitting Copy+Paste too many times.  Patti and Robin Lock both work in the Mathematics Department at St. Lawrence University and they have three children that are statisticians.  They are often referred to as the Lock5.)
-->

To introduce the concept of bootstrapping, we now introduce the `movies` data set in the `ggplot2movies` data frame.  We will load this data frame into R in much the same way as we loaded `flights` and `weather` from the `nycflights13` package.

```{r}
library(ggplot2movies)
data(movies, package = "ggplot2movies")
```

Let's also glance at this data frame using the `View` function and look at the help documentation for `movies`:

```{r eval=FALSE}
View(movies)
?movies
```

We will explore many other features of this data set in the chapters to come, but here we will be focusing on the `rating` variable corresponding to the average IMDB user rating.

You may notice that this data set is quite large:  `r format(nrow(movies), big.mark = ",")` movies have data collected about them here.  This will correspond to our population of ALL movies.  Remember from Chapter \@ref(infer-basics) that our population is rarely known.  We use this data set as our population here to show you the power of bootstrapping in estimating population parameters.  We'll see how **confidence intervals** built using the bootstrap distribution do at including our population parameter of interest.  Here we can actually calculate these values since our population is known, but remember that in general this isn't the case.

Let's take a look at what the distribution of our population `ratings` looks like.  We'll see that we will use the distribution of our sample(s) as an estimate of this population histogram.

```{r fig.cap="Population ratings histogram"}
movies %>% ggplot(aes(x = rating)) +
  geom_histogram(color = "white", bins = 20)
```


***
```{block lc7-3, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why was a histogram chosen as the plot to make for the `rating` variable above?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why does the shape of the `rating` histogram tell us about how IMDB users rate movies?  What stands out about the plot?

***

It's important to think about what our goal is here.  We would like to produce a confidence interval for the population mean `rating`.  We will have to pretend for a moment that we don't have all `r format(nrow(movies), big.mark = ",")` movies.  Let's say that we only have a random sample of 50 movies from this data set instead.  In order to get a random sample, we can use the `sample_n` function from `dplyr`:

```{r}
set.seed(2016)
movies_sample <- movies %>%
  sample_n(50)
```

The `sample_n` function has filtered the data frame `movies` "at random" to choose only 50 rows from the larger `movies` data frame.  We store information on these 50 movies in the `movies_sample` data frame.

Let's now explore what the `rating` variable looks like for these 50 movies:

```{r fig.cap="Sample ratings histogram"}
movies_sample %>% ggplot(aes(x = rating)) +
  geom_histogram(color = "white", bins = 20)
```

Remember that we can think of this histogram as an estimate of our population distribution histogram that we saw above.  We are interested in the population mean rating and trying to find a range of plausible values for that value.  A good start in guessing the population mean is to use the mean of our sample `rating` from the `movies_sample` data:

```{r}
(movies_sample_mean <- movies_sample %>% summarize(mean = mean(rating)))
```

Note the use of the `( )` at the beginning and the end of this creation of the `movies_sample_mean` object.  If you'd like to print out your newly created object, you can enclose it in the parentheses as we have here.

This value of `r movies_sample_mean` is just one guess at the population mean. The idea behind _bootstrapping_ is to sample **with replacement** from the original sample to create new **resamples** of the same size as our original sample.

Returning to our example, let's investigate what one such resample of the `movies_sample` data set accomplishes.  We can create one resample/bootstrap sample by using the `resample` function in the `mosaic` package.  

```{r}
library(mosaic)
library(tibble)
boot1 <- resample(movies_sample, orig.ids = TRUE) %>%
  select(orig.id, everything()) %>%
  arrange(orig.id)
```

Take a look at this resample/bootstrap:

```{r eval=FALSE}
View(boot1)
```


The important thing to note here is the original row numbers from the `movies_sample` data frame in the far left column called `orig.ids`.  Since we are sampling with replacement, there is a strong likelihood that some of the 50 observational units are going to be selected again.

You may be asking yourself what does this mean and how to this lead us to creating a distribution for the sample mean.  Recall that the original sample mean of our data was calculated using the `summarize` function above.

***

```{block lc6-3, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What happens if we change the seed to our pseudo-random generation?  Try it above when we used `sample_n` to describe the resulting `movies_sample`.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why is sampling at random important from the `movies` data frame?  Why don't we just pick `Action` movies and do bootstrapping with this `Action` movies subset?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What was the purpose of assuming we didn't have access to the full `movies` data set here?

***

Before we had a calculated mean in our original sample of `r movies_sample_mean`.  Let's calculate the mean of `ratings` in our
bootstrapped sample:

```{r}
(movies_boot1_mean <- boot1 %>% summarize(mean = mean(rating)))
```

More than likely the calculated bootstrap sample mean is different than the original sample mean.  This is what I meant earlier when I said that the sample means have some variability.  What we are trying to do is replicate many different samples being taken from a larger population.  Our best guess at what the population looks like is multiple copies of the sample we collected.  We then can sample from that larger "created" population by generating bootstrap samples.  

Similar to what we did in the previous section, we can repeat this process using the `do` function followed by an asterisk.  Let's look at 10 different bootstrap means for `ratings` from `movies_sample`.  Note the use of the `resample` function here.

```{r}
do(10) * summarize(resample(movies_sample), mean = mean(rating))
```

You should see some variability begin to tease its way out here.  Many of the simulated means will be close to our original sample mean but many will stray pretty far away.  This occurs because outliers may have been selected a couple of times in the resampling or small values were selected more than larger.  There are myriad reasons why this might be the case.

So what's the next step now?  Just as we repeated the repetitions thousands of times with the "Lady Tasting Tea" example, we can do a similar thing here.

```{r cache=TRUE, fig.cap="Bootstrapped means histogram"}
trials <- do(10000) * summarize(resample(movies_sample), 
                                mean = mean(rating))
trials %>% ggplot(aes(x = mean)) +
  geom_histogram(bins = 30, color = "white")
```

The shape of this resulting distribution may look familiar to you.  It resembles the well-known normal (bell-shaped) curve.  We will see in Chapters \@ref(hypo) and \@ref(ci) when we might expect a normal curve to come through as we have here and when we shouldn't.  There will be specific assumptions that need to be checked and we will see that the normal distribution doesn't always approximate this bootstrapped distribution well.  In those case, we should NOT rely on traditional methods.

At this point, we can easily calculate a confidence interval.  In fact, we have a couple different options.  We will first use the percentiles of the distribution we just created to isolate the middle 95% of values.  This will correspond to our 95% confidence interval for the population mean `rating`, denoted by $\mu$.

```{r}
(ciq_mean_rating <- confint(trials, level = 0.95, method = "quantile"))
```

It's always important at this point to interpret the results of this confidence interval calculation.  In this context, we can say something like the following:

> Based on the sample data and bootstrapping techniques, we can be 95% confident that the true mean rating of ALL IMDB ratings is between `r ciq_mean_rating$lower` and `r ciq_mean_rating$upper`.

This statement may seem a little confusing to you.  Remember that we are pretending like we don't know what the mean IMDB rating for ALL movies is.  Our population here is all of the movies listed in the `movies` data frame from `ggplot2movies`.  So does our bootstrapped confidence interval here contain the actual mean value?

```{r}
summarize(movies, mean = mean(rating))
```

We see here that the population mean does fall in our range of plausible values generated from the bootstrapped samples.

We can also get an idea of how the theory-based inference techniques would have approximated this confidence interval by using the formula $$\bar{x} \pm (2 * SE),$$ where $\bar{x}$ is our original sample mean and $SE$ stands for **standard error** and corresponds to the standard deviation of the bootstrap distribution.  The value of 2 here corresponds to it being a 95% confidence interval.  This formula assumes that the bootstrap distribution is symmetric.  This is often the case with bootstrap distributions, especially those in which the original distribution of the sample is not highly skewed.

To compute this type of confidence interval, we only need to make a slight modification to the `confint` function seen above.  (The expression after the $\pm$ sign is known as the **margin of error**.)

```{r warning=FALSE, message=FALSE}
(cise_mean_rating <- confint(trials, level = 0.95, method = "stderr"))
```

> Based on the sample data and bootstrapping techniques, we can be 95% confident that the true mean rating of ALL IMDB ratings is between `r cise_mean_rating$lower` and `r cise_mean_rating$upper`.


***

```{block lc7-4, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Reproduce the bootstrapping above using a sample of size 50 instead of 25.  What changes do you see?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Reproduce the bootstrapping above using a sample of size 5 instead of 25.  What changes do you see?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** How does the sample size affect the analysis?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why must bootstrap samples be the same size as the original sample?

***

### Review of Bootstrapping

We can summarize the process to generate a bootstrap distribution here in a series of steps that clearly identify the terminology we will use [@lock2012].

- Generate `bootstrap samples` by sampling with replacement from the original sample, using the same sample size.
- Compute the statistic of interest, called a `bootstrap statistic`, for each of the bootstrap samples.
- Collect the statistics for many bootstrap samples to create a `bootstrap distribution`.

Visually, we can represent this process in the following diagram.

```{r bootstrapimg, echo=FALSE, fig.cap="Bootstrapping diagram from Lock5 textbook"}
knitr::include_graphics("images/bootstrap.png")
```

## Relation to hypothesis testing

Recall that we found a statistically significant difference in the sample mean of romance movie ratings compared to the sample mean of action movie ratings.  We concluded Chapter \@ref(hypo) by attempted to understand just how much greater we could expect the _population_ mean romance movie rating to be as compared to the _population_ mean action movie rating.  In order to do so, we will calculate a confidence interval for the difference $\mu_r - \mu_a$.  We'll then go back to our population parameter values and see if our confidence interval contains our parameter value.

We could use bootstrapping in a way similar to that done in the Chapter \@ref(infer-basics), except now on a difference in sample means, to create a distribution and then use the `confint` function with the option of `quantile` to determine a confidence interval for the plausible values of the difference in population means.  This is an excellent programming activity and the reader is urged to try to do so.

Recall what the randomization/null distribution looked like for our simulated shuffled sample means:

```{r fig.cap="Simulated shuffled sample means histogram"}
library(ggplot2)
library(dplyr)
rand_distn %>% ggplot(aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 20)
```


With this null distribution being quite symmetric, the standard error method introduced in Chapter \@ref(infer-basics) likely provides a good estimate of a range of plausible values for $\mu_r - \mu_a$.  Another nice option here is that we can use the standard deviation of the null/randomization distribution we just found with our hypothesis test.

```{r}
(std_err <- rand_distn %>% summarize(se = sd(diffmean)))
```

Remembering that we can use the general formula of $statistic \pm (2 * SE)$ we get the following result for plausible values of the difference in population means at the 95% level.

```{r}
(lower <- obs_diff - (2 * std_err))
(upper <- obs_diff + (2 * std_err))
```

We can, therefore, say that we are 95% confident that the population mean rating for romance movies is between `r round(lower, 3)` and `r round(upper, 3)` points higher than for that of action movies.

The important thing to check here is whether 0 is contained in the confidence interval.  If it is, it is plausible that the difference in the two population means between the two groups is 0.  This means that the null hypothesis is plausible.  The results of the hypothesis test and the confidence interval should match as they do here.  We rejected the null hypothesis with hypothesis testing and we have evidence here than the mean rating for romance movies is higher than for action movies.

## Effect size

The phrase **effect size** has been thrown around recently as an alternative to $p$-values.  In combination with the confidence interval, it can be often more valuable than just looking at the results of a hypothesis test.  It depends on the scientific discipline exactly what is meant by "effect size" but, in general, it refers to _the magnitude of the difference between group measurements_.  For our two sample problem involving movies, it is the observed difference in sample means `obs_diff`.  

It's worthy of mention here that confidence intervals are always centered at the observed statistic.  In other words, if you are looking at a confidence interval and someone asks you what the "effect size" is you can simply find the midpoint of the stated confidence interval.

<!--
## Theory-based CIs (Put into lab instead)
-->

***

```{block lc8-1, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Check to see whether the difference in population mean ratings for the two genres falls in the confidence interval we found here.  Are we guaranteed that it will fall in the range of plausible values?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why do you think many scientific fields are shifting to preferring inclusion of confidence intervals in articles over just $p$-values and hypothesis tests?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  Why is 95% related to a value of 2 in the margin of error?  What would approximate values be for 90% and for 99%?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  Why is a 95% confidence interval wider than a 90% confidence interval?  Explain by using a concrete example from everyday life about what is meant by "confidence."

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  How would confidence intervals correspond to one-sided hypothesis tests?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  There is a relationship between the significance level and the confidence level.  What do you think it is?

***        
  
## Script of R code

```{r include=FALSE, eval=FALSE}
knitr::purl("08-conf_intervals.Rmd", "docs/08-conf_intervals.R")
```

An R script file of all R code used in this chapter is available [here](http://ismayc.github.io/moderndiver-book/08-conf_intervals.R).
  
<!--    
## What's to come? 

We will see in Chapter \@ref(regress) many of the same ideas we have seen with hypothesis testing and confidence intervals in the last two chapters.  Regression is frequently associated both correctly and incorrectly with statistics and data analysis, so you'll need to make sure you understand when it is appropriate and when it is not.
-->
